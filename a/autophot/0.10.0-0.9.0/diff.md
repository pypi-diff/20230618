# Comparing `tmp/autophot-0.10.0-py2.py3-none-any.whl.zip` & `tmp/autophot-0.9.0-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,70 +1,70 @@
-Zip file size: 157223 bytes, number of entries: 83
+Zip file size: 154642 bytes, number of entries: 83
 -rw-rw-r--  2.0 unx     3171 b- defN 23-Jun-13 21:12 autophot/AP_config.py
--rw-rw-r--  2.0 unx     5720 b- defN 23-Jun-18 01:29 autophot/__init__.py
+-rw-rw-r--  2.0 unx     5719 b- defN 23-Jun-13 21:20 autophot/__init__.py
 -rw-rw-r--  2.0 unx      281 b- defN 23-Jun-13 21:12 autophot/__main__.py
 -rw-rw-r--  2.0 unx     1092 b- defN 23-Jun-13 21:12 autophot/fit/__init__.py
 -rw-rw-r--  2.0 unx     6406 b- defN 23-Jun-13 21:12 autophot/fit/base.py
 -rw-rw-r--  2.0 unx       30 b- defN 23-Jun-13 21:12 autophot/fit/gp.py
 -rw-rw-r--  2.0 unx     6961 b- defN 23-Jun-13 21:12 autophot/fit/gradient.py
 -rw-rw-r--  2.0 unx     6939 b- defN 23-Jun-13 21:12 autophot/fit/hmc.py
 -rw-rw-r--  2.0 unx    13401 b- defN 23-Jun-13 21:12 autophot/fit/iterative.py
--rw-rw-r--  2.0 unx    28495 b- defN 23-Jun-18 01:29 autophot/fit/lm.py
+-rw-rw-r--  2.0 unx    31419 b- defN 23-Jun-13 21:12 autophot/fit/lm.py
 -rw-rw-r--  2.0 unx     4381 b- defN 23-Jun-13 21:12 autophot/fit/mhmcmc.py
 -rw-rw-r--  2.0 unx     7150 b- defN 23-Jun-13 21:12 autophot/fit/nuts.py
 -rw-rw-r--  2.0 unx     1150 b- defN 23-Jun-13 21:12 autophot/image/__init__.py
--rw-rw-r--  2.0 unx    16996 b- defN 23-Jun-15 21:05 autophot/image/image_header.py
--rw-rw-r--  2.0 unx    20693 b- defN 23-Jun-15 21:05 autophot/image/image_object.py
+-rw-rw-r--  2.0 unx    16790 b- defN 23-Jun-13 21:12 autophot/image/image_header.py
+-rw-rw-r--  2.0 unx    20545 b- defN 23-Jun-13 21:12 autophot/image/image_object.py
 -rw-rw-r--  2.0 unx     5269 b- defN 23-Jun-13 21:12 autophot/image/jacobian_image.py
 -rw-rw-r--  2.0 unx     6379 b- defN 23-Jun-13 21:12 autophot/image/model_image.py
 -rw-rw-r--  2.0 unx     6791 b- defN 23-Jun-13 21:12 autophot/image/psf_image.py
 -rw-rw-r--  2.0 unx    14763 b- defN 23-Jun-13 21:12 autophot/image/target_image.py
 -rw-rw-r--  2.0 unx    23813 b- defN 23-Jun-13 21:12 autophot/image/window_object.py
--rw-rw-r--  2.0 unx      654 b- defN 23-Jun-17 18:32 autophot/models/__init__.py
--rw-rw-r--  2.0 unx     6441 b- defN 23-Jun-18 01:29 autophot/models/_model_methods.py
--rw-rw-r--  2.0 unx    19427 b- defN 23-Jun-15 21:05 autophot/models/_shared_methods.py
--rw-rw-r--  2.0 unx    12300 b- defN 23-Jun-18 01:29 autophot/models/core_model.py
+-rw-rw-r--  2.0 unx      654 b- defN 23-Jun-13 21:12 autophot/models/__init__.py
+-rw-rw-r--  2.0 unx     1742 b- defN 23-Jun-13 21:12 autophot/models/_model_methods.py
+-rw-rw-r--  2.0 unx    19240 b- defN 23-Jun-13 21:12 autophot/models/_shared_methods.py
+-rw-rw-r--  2.0 unx    12502 b- defN 23-Jun-13 21:12 autophot/models/core_model.py
 -rw-rw-r--  2.0 unx     6850 b- defN 23-Jun-13 21:12 autophot/models/edgeon_model.py
 -rw-rw-r--  2.0 unx    14419 b- defN 23-Jun-13 21:12 autophot/models/exponential_model.py
 -rw-rw-r--  2.0 unx     2108 b- defN 23-Jun-13 21:12 autophot/models/flatsky_model.py
 -rw-rw-r--  2.0 unx    10650 b- defN 23-Jun-13 21:12 autophot/models/foureirellipse_model.py
--rw-rw-r--  2.0 unx     5239 b- defN 23-Jun-15 21:05 autophot/models/galaxy_model_object.py
+-rw-rw-r--  2.0 unx     5306 b- defN 23-Jun-13 21:12 autophot/models/galaxy_model_object.py
 -rw-rw-r--  2.0 unx    13246 b- defN 23-Jun-13 21:12 autophot/models/gaussian_model.py
--rw-rw-r--  2.0 unx    10888 b- defN 23-Jun-18 01:29 autophot/models/group_model_object.py
--rw-rw-r--  2.0 unx    22913 b- defN 23-Jun-18 01:29 autophot/models/model_object.py
+-rw-rw-r--  2.0 unx    11139 b- defN 23-Jun-13 21:12 autophot/models/group_model_object.py
+-rw-rw-r--  2.0 unx    23870 b- defN 23-Jun-13 21:12 autophot/models/model_object.py
 -rw-rw-r--  2.0 unx     3904 b- defN 23-Jun-13 21:12 autophot/models/moffat_model.py
 -rw-rw-r--  2.0 unx    18951 b- defN 23-Jun-13 21:12 autophot/models/nuker_model.py
--rw-rw-r--  2.0 unx    12779 b- defN 23-Jun-18 01:29 autophot/models/parameter_group.py
--rw-rw-r--  2.0 unx    18591 b- defN 23-Jun-18 01:29 autophot/models/parameter_object.py
+-rw-rw-r--  2.0 unx    12454 b- defN 23-Jun-13 21:12 autophot/models/parameter_group.py
+-rw-rw-r--  2.0 unx    18097 b- defN 23-Jun-13 21:12 autophot/models/parameter_object.py
 -rw-rw-r--  2.0 unx     2450 b- defN 23-Jun-13 21:12 autophot/models/planesky_model.py
--rw-rw-r--  2.0 unx     3888 b- defN 23-Jun-15 21:05 autophot/models/psf_model.py
+-rw-rw-r--  2.0 unx     3719 b- defN 23-Jun-13 21:12 autophot/models/psf_model.py
 -rw-rw-r--  2.0 unx     4998 b- defN 23-Jun-13 21:12 autophot/models/ray_model.py
--rw-rw-r--  2.0 unx    16161 b- defN 23-Jun-15 14:56 autophot/models/sersic_model.py
+-rw-rw-r--  2.0 unx    16161 b- defN 23-Jun-13 21:12 autophot/models/sersic_model.py
 -rw-rw-r--  2.0 unx      875 b- defN 23-Jun-13 21:12 autophot/models/sky_model_object.py
 -rw-rw-r--  2.0 unx    10695 b- defN 23-Jun-13 21:12 autophot/models/spline_model.py
--rw-rw-r--  2.0 unx     1159 b- defN 23-Jun-18 01:29 autophot/models/star_model_object.py
--rw-rw-r--  2.0 unx     3231 b- defN 23-Jun-15 21:05 autophot/models/superellipse_model.py
+-rw-rw-r--  2.0 unx     1226 b- defN 23-Jun-13 21:12 autophot/models/star_model_object.py
+-rw-rw-r--  2.0 unx     3259 b- defN 23-Jun-13 21:12 autophot/models/superellipse_model.py
 -rw-rw-r--  2.0 unx     4741 b- defN 23-Jun-13 21:12 autophot/models/warp_model.py
 -rw-rw-r--  2.0 unx     3981 b- defN 23-Jun-13 21:12 autophot/models/wedge_model.py
 -rw-rw-r--  2.0 unx       57 b- defN 23-Jun-13 21:12 autophot/parse_config/__init__.py
 -rw-rw-r--  2.0 unx     4147 b- defN 23-Jun-13 21:12 autophot/parse_config/basic_config.py
 -rw-rw-r--  2.0 unx     4590 b- defN 23-Jun-13 21:12 autophot/parse_config/galfit_config.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 21:12 autophot/parse_config/shared_methods.py
 -rw-rw-r--  2.0 unx       67 b- defN 23-Jun-13 21:12 autophot/plots/__init__.py
--rw-rw-r--  2.0 unx    14203 b- defN 23-Jun-18 01:29 autophot/plots/image.py
+-rw-rw-r--  2.0 unx     8074 b- defN 23-Jun-13 21:12 autophot/plots/image.py
 -rw-rw-r--  2.0 unx     7556 b- defN 23-Jun-13 21:12 autophot/plots/profile.py
 -rw-rw-r--  2.0 unx     3048 b- defN 23-Jun-13 21:12 autophot/plots/shared_elements.py
 -rw-rw-r--  2.0 unx    21022 b- defN 23-Jun-13 21:12 autophot/plots/visuals.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 21:12 autophot/utils/__init__.py
 -rw-rw-r--  2.0 unx      800 b- defN 23-Jun-13 21:12 autophot/utils/angle_operations.py
 -rw-rw-r--  2.0 unx      976 b- defN 23-Jun-13 21:12 autophot/utils/decorators.py
--rw-rw-r--  2.0 unx    11683 b- defN 23-Jun-15 21:05 autophot/utils/interpolate.py
--rw-rw-r--  2.0 unx     5738 b- defN 23-Jun-15 21:05 autophot/utils/operations.py
--rw-rw-r--  2.0 unx      964 b- defN 23-Jun-15 21:05 autophot/utils/optimization.py
--rw-rw-r--  2.0 unx     5856 b- defN 23-Jun-15 21:05 autophot/utils/parametric_profiles.py
+-rw-rw-r--  2.0 unx     9815 b- defN 23-Jun-13 21:12 autophot/utils/interpolate.py
+-rw-rw-r--  2.0 unx     6728 b- defN 23-Jun-13 21:12 autophot/utils/operations.py
+-rw-rw-r--  2.0 unx      963 b- defN 23-Jun-13 21:12 autophot/utils/optimization.py
+-rw-rw-r--  2.0 unx     5990 b- defN 23-Jun-13 21:12 autophot/utils/parametric_profiles.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 21:12 autophot/utils/conversions/__init__.py
 -rw-rw-r--  2.0 unx     1840 b- defN 23-Jun-13 21:12 autophot/utils/conversions/coordinates.py
 -rw-rw-r--  2.0 unx     1095 b- defN 23-Jun-13 21:12 autophot/utils/conversions/dict_to_hdf5.py
 -rw-rw-r--  2.0 unx     1829 b- defN 23-Jun-13 21:12 autophot/utils/conversions/functions.py
 -rw-rw-r--  2.0 unx     3260 b- defN 23-Jun-13 21:12 autophot/utils/conversions/optimization.py
 -rw-rw-r--  2.0 unx     2539 b- defN 23-Jun-13 21:12 autophot/utils/conversions/units.py
 -rw-rw-r--  2.0 unx      281 b- defN 23-Jun-13 21:12 autophot/utils/initialize/__init__.py
@@ -72,14 +72,14 @@
 -rw-rw-r--  2.0 unx     4542 b- defN 23-Jun-13 21:12 autophot/utils/initialize/construct_psf.py
 -rw-rw-r--  2.0 unx     3921 b- defN 23-Jun-13 21:12 autophot/utils/initialize/initialize.py
 -rw-rw-r--  2.0 unx     7036 b- defN 23-Jun-13 21:12 autophot/utils/initialize/segmentation_map.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 21:12 autophot/utils/isophote/__init__.py
 -rw-rw-r--  2.0 unx     1085 b- defN 23-Jun-13 21:12 autophot/utils/isophote/ellipse.py
 -rw-rw-r--  2.0 unx     8531 b- defN 23-Jun-13 21:12 autophot/utils/isophote/extract.py
 -rw-rw-r--  2.0 unx     7012 b- defN 23-Jun-13 21:12 autophot/utils/isophote/integrate.py
--rw-rw-r--  2.0 unx    35149 b- defN 23-Jun-18 01:30 autophot-0.10.0.dist-info/LICENSE
--rw-rw-r--  2.0 unx     3557 b- defN 23-Jun-18 01:30 autophot-0.10.0.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 23-Jun-18 01:30 autophot-0.10.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       56 b- defN 23-Jun-18 01:30 autophot-0.10.0.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        9 b- defN 23-Jun-18 01:30 autophot-0.10.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     7278 b- defN 23-Jun-18 01:30 autophot-0.10.0.dist-info/RECORD
-83 files, 584360 bytes uncompressed, 145673 bytes compressed:  75.1%
+-rw-rw-r--  2.0 unx    35149 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     3572 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       56 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        9 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7270 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/RECORD
+83 files, 575760 bytes uncompressed, 143104 bytes compressed:  75.1%
```

## zipnote {}

```diff
@@ -225,26 +225,26 @@
 
 Filename: autophot/utils/isophote/extract.py
 Comment: 
 
 Filename: autophot/utils/isophote/integrate.py
 Comment: 
 
-Filename: autophot-0.10.0.dist-info/LICENSE
+Filename: autophot-0.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: autophot-0.10.0.dist-info/METADATA
+Filename: autophot-0.9.0.dist-info/METADATA
 Comment: 
 
-Filename: autophot-0.10.0.dist-info/WHEEL
+Filename: autophot-0.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: autophot-0.10.0.dist-info/entry_points.txt
+Filename: autophot-0.9.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: autophot-0.10.0.dist-info/top_level.txt
+Filename: autophot-0.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: autophot-0.10.0.dist-info/RECORD
+Filename: autophot-0.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## autophot/__init__.py

```diff
@@ -1,15 +1,15 @@
 import sys
 import argparse
 import requests
 from .parse_config import galfit_config, basic_config
 from . import models, image, plots, utils, fit, AP_config
 
 # meta data
-__version__ = "0.10.0"
+__version__ = "0.9.0"
 __author__ = "Connor Stone"
 __email__ = "connorstone628@gmail.com"
 
 
 def run_from_terminal() -> None:
     """
     Execute AutoPhot from the command line with various options.
```

## autophot/fit/lm.py

```diff
@@ -106,14 +106,15 @@
         if self.model.target.has_mask:
             self.mask = self.model.target[self.fit_window].flatten("mask")
             # subtract masked pixels from degrees of freedom
             self.ndf -= torch.sum(self.mask)
         self.L_history = []
         self.decision_history = []
         self.rho_history = []
+        self._count_grad_step = 0
         self._count_converged = 0
         self.ndf = kwargs.get("ndf", self.ndf)
         self._covariance_matrix = None
 
         # update attributes with constraints
         self.constraints = kwargs.get("constraints", None)
         if self.constraints is not None and isinstance(self.constraints, LM_Constraint):
@@ -138,14 +139,74 @@
         self.L = min(1e9, self.L * Lup)
 
     def L_dn(self, Ldn=None):
         if Ldn is None:
             Ldn = self.Ldn
         self.L = max(1e-9, self.L / Ldn)
 
+    @torch.no_grad()
+    def grad_step(self) -> None:
+        L = 0.1
+        self.iteration += 1
+        self._count_grad_step += 1
+        if self.verbose > 1:
+            AP_config.ap_logger.info(
+                f"taking grad step. Loss to beat: {np.nanmin(self.loss_history[:-1])}"
+            )
+        for count in range(20):
+            self.update_Yp(self.grad * L)
+            loss = self.update_chi2()
+
+            if not torch.isfinite(loss):
+                L /= 10
+                continue
+            if self.verbose > 1:
+                AP_config.ap_logger.info(f"grad step loss: {loss.item()}, L: {L}")
+            if np.nanmin(self.loss_history[:-1]) > loss.item():
+                self.loss_history.append(loss.detach().cpu().item())
+                self.L = 1.0
+                self.L_history.append(self.L)
+                self.current_state += self.grad * L
+                self.lambda_history.append(
+                    np.copy(self.current_state.detach().cpu().numpy())
+                )
+                self.decision_history.append("accept grad")
+                if self.verbose > 0:
+                    AP_config.ap_logger.info("accept grad")
+                self.rho_history.append(1.0)
+                self.prev_Y[0] = self.prev_Y[1]
+                self.prev_Y[1] = torch.clone(self.current_Y)
+                break
+            elif (
+                np.abs(np.nanmin(self.loss_history[:-1]) - loss.item())
+                < (self.relative_tolerance * 1e-3)
+                and L < 1e-5
+            ):
+                self.loss_history.append(loss.detach().cpu().item())
+                self.L = 1.0
+                self.L_history.append(self.L)
+                self.current_state += self.grad * L
+                self.lambda_history.append(
+                    np.copy(self.current_state.detach().cpu().numpy())
+                )
+                self.decision_history.append("accept bad grad")
+                if self.verbose > 0:
+                    AP_config.ap_logger.info("accept bad grad")
+                self.rho_history.append(1.0)
+                self.prev_Y[0] = self.prev_Y[1]
+                self.prev_Y[1] = torch.clone(self.current_Y)
+                break
+            else:
+                L /= 10
+                continue
+        else:
+            raise RuntimeError(
+                "Unable to take gradient step! LM has found itself in a very bad place of parameter space, try adjusting initial parameters"
+            )
+
     def step(self, current_state=None) -> None:
         """
         Levenberg-Marquardt update step
         """
         if current_state is not None:
             self.current_state = current_state
 
@@ -179,41 +240,46 @@
             self.decision_history.append("nan")
             self.rho_history.append(None)
             self._count_reject += 1
             self.iteration += 1
             self.L_up()
             return
         elif self.iteration > 0:
-            lossmin = np.nanmin(self.loss_history[:-1])
-            rho = self.rho(lossmin, loss, h)
+            rho = self.rho(np.nanmin(self.loss_history[:-1]), loss, h)
             if self.verbose > 1:
                 AP_config.ap_logger.debug(
                     f"LM loss: {loss.item()}, best loss: {np.nanmin(self.loss_history[:-1])}, loss diff: {np.nanmin(self.loss_history[:-1]) - loss.item()}, L: {self.L}"
                 )
             self.rho_history.append(rho)
             if self.verbose > 1:
                 AP_config.ap_logger.debug(f"rho: {rho.item()}")
 
-            if rho > self.epsilon4 or (all(torch.abs(self.grad).detach().cpu().numpy() < self.relative_tolerance) and np.abs((lossmin - loss.item()) / lossmin) < self.relative_tolerance):
+            if rho > self.epsilon4:
                 if self.verbose > 0:
                     AP_config.ap_logger.info("accept")
                 self.decision_history.append("accept")
                 self.prev_Y[0] = self.prev_Y[1]
                 self.prev_Y[1] = torch.clone(self.current_Y)
                 self.current_state += h
                 self.L_dn()
                 self._count_reject = 0
                 if (
                     0
-                    < (self.ndf * (lossmin - loss) / loss)
+                    < (self.ndf * (np.nanmin(self.loss_history[:-1]) - loss) / loss)
                     < self.relative_tolerance
                 ):
                     self._count_finish += 1
                 else:
                     self._count_finish = 0
+            # elif self._count_reject == 4:
+            #     if self.verbose > 0:
+            #         AP_config.ap_logger.info("reject, resetting jacobian")
+            #     self.decision_history.append("reject")
+            #     self.L = min(1e-2, self.L / self.Lup**4)
+            #     self._count_reject += 1
             else:
                 if self.verbose > 0:
                     AP_config.ap_logger.info("reject")
                 self.decision_history.append("reject")
                 self.L_up()
                 self._count_reject += 1
                 return
@@ -276,51 +342,59 @@
                 if (
                     self.decision_history.count("accept") > 2
                     and self.decision_history[-1] == "accept"
                     and L[-1] < 0.1
                     and ((loss[-2] - loss[-1]) / loss[-1])
                     < (self.relative_tolerance / 10)
                 ):
+                    self._count_grad_step = 0
                     self._count_converged += 1
+                elif self._count_grad_step >= 5:
+                    self.message = (
+                        self.message
+                        + "success by immobility, unable to find improvement either converged or bad area of parameter space."
+                    )
+                    break
                 elif self.iteration >= self.max_iter:
                     self.message = (
                         self.message + f"fail max iterations reached: {self.iteration}"
                     )
                     break
                 elif not torch.all(torch.isfinite(self.current_state)):
                     self.message = self.message + "fail non-finite step taken"
                     break
                 elif (
                     self.L >= (1e9 - 1)
-                    and self._count_reject >= 8
+                    and self._count_reject >= 12
                     and not self.take_low_rho_step()
                 ):
-                    self.message = (
-                        self.message
-                        + "fail by immobility, unable to find improvement or even small bad step"
-                    )
-                    break
+                    if not self.full_jac:
+                        self.update_J_AD()
+                        self.update_hess()
+                        self.update_grad(self.prev_Y[1])
+                    try:
+                        self.grad_step()
+                    except RuntimeError:
+                        self.message = (
+                            self.message
+                            + "fail by immobility, unable to find improvement or even small bad step"
+                        )
+                        break
                 if self._count_converged >= 2:
                     self.message = self.message + "success"
                     break
                 lam, L, loss = self.accept_history()
                 if len(loss) >= 10:
                     loss10 = np.array(loss[-10:])
                     if np.all(
-                        np.abs((loss10[0] - loss10[-1]) / loss10[-1])
+                        np.abs((loss10[1:] - loss10[:-1]) / loss10[:-1])
                         < self.relative_tolerance
-                    ) and L[-1] < 0.1:
+                    ):
                         self.message = self.message + "success"
                         break
-                    if np.all(
-                        np.abs((loss10[0] - loss10[-1]) / loss10[-1])
-                        < self.relative_tolerance
-                    ) and L[-1] >= 0.1:
-                        self.message = self.message + "fail by immobility, possible bad area of parameter space."
-                        break
         except KeyboardInterrupt:
             self.message = self.message + "fail interrupted"
 
         if self.message.startswith("fail") and self._count_finish > 0:
             self.message = (
                 self.message
                 + ". possibly converged to numerical precision and could not make a better step."
@@ -339,15 +413,15 @@
 
     def update_uncertainty(self):
         # set the uncertainty for each parameter
         cov = self.covariance_matrix
         if torch.all(torch.isfinite(cov)):
             try:
                 self.model.parameters.set_uncertainty(
-                    2*torch.sqrt(torch.abs(torch.diag(cov))), # fixme, is factor "2" too conservative?
+                    torch.sqrt(torch.abs(torch.diag(cov))),
                     as_representation=False,
                     parameters_identity=self.fit_parameters_identity,
                 )
             except RuntimeError as e:
                 AP_config.ap_logger.warning(f"Unable to update uncertainty due to: {e}")
 
     @torch.no_grad()
@@ -418,28 +492,28 @@
         if self.iteration == 0:
             return h
 
         h = torch.linalg.solve(
             (
                 self.hess
                 + 1e-3
-                * self.L**2
+                * self.L
                 * torch.eye(
                     len(self.grad), dtype=AP_config.ap_dtype, device=AP_config.ap_device
                 )
             )
             * (
                 1
-                + self.L**2
+                + self.L
                 * torch.eye(
                     len(self.grad), dtype=AP_config.ap_dtype, device=AP_config.ap_device
                 )
             )
             ** 2
-            / (1 + self.L**2),
+            / (1 + self.L),
             self.grad,
         )
         return h
 
     @torch.no_grad()
     def update_Yp(self, h):
         """
```

## autophot/image/image_header.py

```diff
@@ -91,17 +91,16 @@
                         data_shape, dtype=AP_config.ap_dtype, device=AP_config.ap_device
                     ),
                     (0,),
                 )
             ))
             shape = torch.linalg.solve(self.pixelscale / self.pixel_length, end)
             if wcs is not None:
-                wcs_origin = wcs.pixel_to_world(-0.5, -0.5)
                 origin = torch.as_tensor(
-                   [wcs_origin.ra.arcsec, wcs_origin.dec.arcsec] , dtype=AP_config.ap_dtype, device=AP_config.ap_device
+                    wcs.pixel_to_world(-0.5, -0.5), dtype=AP_config.ap_dtype, device=AP_config.ap_device
                 )
             elif origin is None and center is None:
                 origin = torch.zeros(
                     2, dtype=AP_config.ap_dtype, device=AP_config.ap_device
                 )
             elif center is None:
                 origin = torch.as_tensor(
@@ -160,23 +159,19 @@
         transformed into the world coordiante system based on the
         pixel scale and origin position for this image. In the world
         coordinate system the origin is placed with respect to the
         bottom corner of the 0,0 pixel.
 
         """
         if internal_transpose:
-            return (self.pixelscale @ pixel_coordinate).T + self.pixel_origin
-        return (self.pixelscale @ pixel_coordinate) + self.pixel_origin
+            return (self.pixelscale @ (pixel_coordinate)).T + self.pixel_origin
+        return (self.pixelscale @ (pixel_coordinate)) + self.pixel_origin
 
-    def world_to_pixel(self, world_coordinate, unsqueeze_origin = False):
-        if unsqueeze_origin:
-            O = self.pixel_origin.unsqueeze(-1)
-        else:
-            O = self.pixel_origin
-        return torch.linalg.solve(self.pixelscale, world_coordinate - O)
+    def world_to_pixel(self, world_coordinate):
+        return torch.linalg.solve(self.pixelscale, world_coordinate - self.pixel_origin)
 
     def pixel_to_world_delta(self, pixel_delta):
         """Take in a coordinate on the regular cartesian pixel grid, where
         0,0 is the center of the first pixel. This coordinate is
         transformed into the world coordiante system based on the
         pixel scale and origin position for this image. In the world
         coordinate system the origin is placed with respect to the
```

## autophot/image/image_object.py

```diff
@@ -109,16 +109,16 @@
     @property
     def pixel_length(self):
         return self.header.pixel_length
     
     def pixel_to_world(self, pixel_coordinate, internal_transpose = False):
         return self.header.pixel_to_world(pixel_coordinate, internal_transpose = internal_transpose)
 
-    def world_to_pixel(self, world_coordinate, unsqueeze_origin = False):
-        return self.header.world_to_pixel(world_coordinate, unsqueeze_origin)
+    def world_to_pixel(self, world_coordinate):
+        return self.header.world_to_pixel(world_coordinate)
     
     def pixel_to_world_delta(self, pixel_coordinate):
         return self.header.pixel_to_world_delta(pixel_coordinate)
 
     def world_to_pixel_delta(self, world_coordinate):
         return self.header.world_to_pixel_delta(world_coordinate)
     
@@ -283,16 +283,14 @@
     def flatten(self, attribute: str = "data") -> np.ndarray:
         return getattr(self, attribute).reshape(-1)
 
     def get_coordinate_meshgrid(self):
         return self.header.get_coordinate_meshgrid()
     def get_coordinate_corner_meshgrid(self):
         return self.header.get_coordinate_corner_meshgrid()
-    def get_coordinate_simps_meshgrid(self):
-        return self.header.get_coordinate_simps_meshgrid()
 
     def reduce(self, scale: int, **kwargs):
         """This operation will downsample an image by the factor given. If
         scale = 2 then 2x2 blocks of pixels will be summed together to
         form individual larger pixels. A new image object will be
         returned with the appropriate pixelscale and data tensor. Note
         that the window does not change in this operation since the
```

## autophot/models/_model_methods.py

```diff
@@ -1,21 +1,13 @@
-from typing import Optional, Union, Dict, Tuple, Any
-from copy import deepcopy
-
 import numpy as np
 import torch
-
+from typing import Optional, Union, Dict, Tuple, Any
+from copy import deepcopy
 from .parameter_object import Parameter
-from ..utils.interpolate import _shift_Lanczos_kernel_torch, simpsons_kernel, curvature_kernel
 from ..image import Model_Image, Target_Image, Window
-from ..utils.operations import (
-    fft_convolve_torch,
-    fft_convolve_multi_torch,
-    grid_integrate,
-)
 from .. import AP_config
 
 
 @classmethod
 def build_parameter_specs(cls, user_specs=None):
     parameter_specs = {}
     for base in cls.__bases__:
@@ -48,116 +40,7 @@
         # If a parameter object is provided, simply use as-is
         if isinstance(self.parameter_specs[p], Parameter):
             self.parameters.add_parameter(self.parameter_specs[p].to())
         elif isinstance(self.parameter_specs[p], dict):
             self.parameters.add_parameter(Parameter(p, **self.parameter_specs[p]))
         else:
             raise ValueError(f"unrecognized parameter specification for {p}")
-
-
-def _sample_init(self, image, parameters, center):
-    if self.sampling_mode == "midpoint" and max(image.data.shape) >= 100:
-        Coords = image.get_coordinate_meshgrid()
-        X, Y = Coords - center[...,None,None]
-        mid = self.evaluate_model(
-            X = X, Y = Y,
-            image=image, parameters=parameters
-        )
-        kernel = curvature_kernel(AP_config.ap_dtype, AP_config.ap_device)
-        curvature = torch.nn.functional.pad(torch.nn.functional.conv2d(
-            mid.view(1, 1, *mid.shape),
-            kernel.view(1, 1, *kernel.shape),
-            padding="valid",
-        ), (1,1,1,1), mode = "replicate").squeeze()
-        return mid + curvature, mid            
-    elif self.sampling_mode == "trapezoid" and max(image.data.shape) >= 100:
-        Coords = image.get_coordinate_corner_meshgrid()
-        X, Y = Coords - center[...,None,None]
-        dens = self.evaluate_model(
-            X = X, Y = Y,
-            image=image, parameters=parameters
-        )
-        kernel = torch.ones((1,1,2,2), dtype = AP_config.ap_dtype, device = AP_config.ap_device) / 4.
-        trapz = torch.nn.functional.conv2d(dens.view(1,1,*dens.shape), kernel, padding="valid")
-        trapz = trapz.squeeze()
-        kernel = curvature_kernel(AP_config.ap_dtype, AP_config.ap_device)
-        curvature = torch.nn.functional.pad(torch.nn.functional.conv2d(
-            trapz.view(1, 1, *trapz.shape),
-            kernel.view(1, 1, *kernel.shape),
-            padding="valid",
-        ), (1,1,1,1), mode = "replicate").squeeze()
-        return trapz + curvature, trapz
-            
-    Coords = image.get_coordinate_simps_meshgrid()
-    X, Y = Coords - center[...,None,None]
-    dens = self.evaluate_model(
-        X = X, Y = Y,
-        image=image, parameters=parameters
-    )
-    kernel = simpsons_kernel(dtype = AP_config.ap_dtype, device = AP_config.ap_device)
-    mid = torch.nn.functional.conv2d(dens.view(1,1,*dens.shape), torch.ones_like(kernel) / 9, stride = 2, padding="valid") #dens[1::2,1::2]
-    simps = torch.nn.functional.conv2d(dens.view(1,1,*dens.shape), kernel, stride = 2, padding="valid")
-    return mid.squeeze(), simps.squeeze()
-
-def _sample_integrate(self, deep, reference, image, parameters, center):
-    if self.integrate_mode == "none":
-        pass
-    elif self.integrate_mode == "threshold":
-        Coords = image.get_coordinate_meshgrid()
-        X, Y = Coords - center[...,None, None]
-        ref = torch.sum(deep) / deep.numel()
-        error = torch.abs((deep - reference))
-        select = error > (self.sampling_tolerance*ref)
-        intdeep = grid_integrate(
-            X=X[select],
-            Y=Y[select],
-            value = deep[select],
-            compare = reference[select],
-            image_header=image.header,
-            eval_brightness=self.evaluate_model,
-            eval_parameters=parameters,
-            dtype=AP_config.ap_dtype,
-            device=AP_config.ap_device,
-            tolerance=self.sampling_tolerance,
-            reference=ref,
-        )
-        deep[select] = intdeep
-    else:
-        raise ValueError(
-            f"{self.name} has unknown integration mode: {self.integrate_mode}"
-        )
-    return deep
-
-def _sample_convolve(self, image, shift, psf):
-    if shift is not None:
-        if any(np.array(psf.data.shape) < 10):
-            psf_data = torch.nn.functional.pad(psf.data, (2,2,2,2))
-        else:
-            psf_data = psf.data
-        pix_center_shift = image.world_to_pixel_delta(shift)
-        LL = _shift_Lanczos_kernel_torch(
-            -pix_center_shift[0],
-            -pix_center_shift[1],
-            2,
-            AP_config.ap_dtype,
-            AP_config.ap_device,
-        )
-        shift_psf = torch.nn.functional.conv2d(
-            psf_data.view(1, 1, *psf_data.shape),
-            LL.view(1, 1, *LL.shape),
-            padding="valid",
-        ).squeeze()
-    else:
-        shift_psf = psf.data
-        
-    if self.psf_convolve_mode == "fft":
-        image.data = fft_convolve_torch(
-            image.data, shift_psf / torch.sum(shift_psf), img_prepadded=True
-        )
-    elif self.psf_convolve_mode == "direct":
-        image.data = torch.nn.functional.conv2d(
-            image.data.view(1, 1, *image.data.shape),
-            torch.flip(shift_psf.view(1, 1, *shift_psf.shape) / torch.sum(shift_psf), dims = (2,3)),
-            padding="same",
-        ).squeeze()
-    else:
-        raise ValueError(f"unrecognized psf_convolve_mode: {self.psf_convolve_mode}")
```

## autophot/models/_shared_methods.py

```diff
@@ -265,15 +265,15 @@
 
 
 # Exponential
 ######################################################################
 @default_internal
 def exponential_radial_model(self, R, image=None, parameters=None):
     return exponential_torch(
-        R + self.softening,
+        R,
         parameters["Re"].value,
         (10 ** parameters["Ie"].value) * image.pixel_area,
     )
 
 
 @default_internal
 def exponential_iradial_model(self, i, R, image=None, parameters=None):
@@ -285,94 +285,94 @@
 
 
 # Sersic
 ######################################################################
 @default_internal
 def sersic_radial_model(self, R, image=None, parameters=None):
     return sersic_torch(
-        R + self.softening,
+        R,
         parameters["n"].value,
         parameters["Re"].value,
         (10 ** parameters["Ie"].value) * image.pixel_area,
     )
 
 
 @default_internal
 def sersic_iradial_model(self, i, R, image=None, parameters=None):
     return sersic_torch(
-        R + self.softening,
+        R,
         parameters["n"].value[i],
         parameters["Re"].value[i],
         (10 ** parameters["Ie"].value[i]) * image.pixel_area,
     )
 
 
 # Moffat
 ######################################################################
 @default_internal
 def moffat_radial_model(self, R, image=None, parameters=None):
     return moffat_torch(
-        R + self.softening,
+        R,
         parameters["n"].value,
         parameters["Rd"].value,
         (10 ** parameters["I0"].value) * image.pixel_area,
     )
 
 
 @default_internal
 def moffat_iradial_model(self, i, R, image=None, parameters=None):
     return moffat_torch(
-        R + self.softening,
+        R,
         parameters["n"].value[i],
         parameters["Rd"].value[i],
         (10 ** parameters["I0"].value[i]) * image.pixel_area,
     )
 
 
 # Nuker Profile
 ######################################################################
 @default_internal
 def nuker_radial_model(self, R, image=None, parameters=None):
     return nuker_torch(
-        R + self.softening,
+        R,
         parameters["Rb"].value,
         (10 ** parameters["Ib"].value) * image.pixel_area,
         parameters["alpha"].value,
         parameters["beta"].value,
         parameters["gamma"].value,
     )
 
 
 @default_internal
 def nuker_iradial_model(self, i, R, image=None, parameters=None):
     return nuker_torch(
-        R + self.softening,
+        R,
         parameters["Rb"].value[i],
         (10 ** parameters["Ib"].value[i]) * image.pixel_area,
         parameters["alpha"].value[i],
         parameters["beta"].value[i],
         parameters["gamma"].value[i],
     )
 
 
 # Gaussian
 ######################################################################
 @default_internal
 def gaussian_radial_model(self, R, image=None, parameters=None):
     return gaussian_torch(
-        R + self.softening,
+        R,
         parameters["sigma"].value,
         (10 ** parameters["flux"].value) * image.pixel_area,
     )
 
 
 @default_internal
 def gaussian_iradial_model(self, i, R, image=None, parameters=None):
     return gaussian_torch(
-        R + self.softening,
+        R,
         parameters["sigma"].value[i],
         (10 ** parameters["flux"].value[i]) * image.pixel_area,
     )
 
 
 # Spline
 ######################################################################
@@ -520,24 +520,24 @@
             S / (np.abs(I) * np.log(10)), override_locked=True, index=s
         )
 
 
 @default_internal
 def spline_radial_model(self, R, image=None, parameters=None):
     return spline_torch(
-        R + self.softening,
+        R,
         parameters["I(R)"].prof,
         parameters["I(R)"].value,
         image.pixel_area,
         extend=self.extend_profile,
     )
 
 
 @default_internal
 def spline_iradial_model(self, i, R, image=None, parameters=None):
     return spline_torch(
-        R + self.softening,
+        R,
         parameters["I(R)"].prof,
         parameters["I(R)"].value[i],
         image.pixel_area,
         extend=self.extend_profile,
     )
```

## autophot/models/core_model.py

```diff
@@ -73,32 +73,37 @@
     def __init__(self, name, *args, target=None, window=None, locked=False, **kwargs):
         assert (
             ":" not in name and "|" not in name
         ), "characters '|' and ':' are reserved for internal model operations please do not include these in a model name"
         self.name = name
         AP_config.ap_logger.debug("Creating model named: {self.name}")
         self.constraints = kwargs.get("constraints", None)
+        self.equality_constraints = []
         self.parameters = Parameter_Group(self.name)
+        self.requires_grad = kwargs.get("requires_grad", False)
         self.target = target
         self.window = window
         self._locked = locked
         self.mask = kwargs.get("mask", None)
-        
+
     def add_equality_constraint(self, model, parameter):
         if isinstance(parameter, (tuple, list)):
             for P in parameter:
                 self.add_equality_constraint(model, P)
             return
         del_param = self.parameters.get_name(parameter)
-        old_groups = del_param.groups
         use_param = model.parameters.get_name(parameter)
+        old_groups = del_param.groups
         for group in old_groups:
             group.pop_id(del_param.identity)
             group.add_parameter(use_param)
 
+        self.equality_constraints.append(parameter)
+        model.equality_constraints.append(parameter)
+
     @torch.no_grad()
     @ignore_numpy_warnings
     @select_target
     @default_internal
     def initialize(self, target=None, parameters=None, **kwargs):
         """When this function finishes, all parameters should have numerical
         values (non None) that are reasonable estimates of the final
@@ -349,8 +354,9 @@
         elif isinstance(parameters, torch.Tensor):
             self.parameters.set_values(
                 parameters,
                 as_representation=as_representation,
                 parameters_identity=parameters_identity,
             )
             parameters = self.parameters
+
         return self.sample(image=image, window=window, parameters=parameters, **kwargs)
```

## autophot/models/galaxy_model_object.py

```diff
@@ -110,16 +110,16 @@
                 q_samples[np.argmin(list(iso["amplitude2"] for iso in iso_info))],
                 override_locked=True,
             )
 
     @default_internal
     def radius_metric(self, X, Y, image=None, parameters=None):
         return torch.sqrt(
-            (torch.abs(X)) ** 2 + (torch.abs(Y)) ** 2
-        )
+            (torch.abs(X) + 1e-8) ** 2 + (torch.abs(Y) + 1e-8) ** 2
+        )  # epsilon added for numerical stability of gradient
 
     @default_internal
     def transform_coordinates(self, X, Y, image=None, parameters=None):
         X, Y = Rotate_Cartesian(-(parameters["PA"].value - image.north), X, Y)
         return (
             X,
             Y / parameters["q"].value,
```

## autophot/models/group_model_object.py

```diff
@@ -75,14 +75,25 @@
                 f"{self.name} already has model with name {model.name}, every model must have a unique name."
             )
 
         self.models[model.name] = model
         self.parameters.add_group(model.parameters)
         self.update_window()
 
+    @property
+    def equality_constraints(self):
+        try:
+            return self._equality_constraints
+        except AttributeError:
+            return []
+
+    @equality_constraints.setter
+    def equality_constraints(self, val):
+        pass
+
     def update_window(self, include_locked: bool = False):
         """Makes a new window object which encloses all the windows of the
         sub models in this group model object.
 
         """
         if isinstance(
             self.target, Image_List
```

## autophot/models/model_object.py

```diff
@@ -12,17 +12,17 @@
 from .parameter_object import Parameter
 from .parameter_group import Parameter_Group
 from ..utils.initialize import center_of_mass
 from ..utils.decorators import ignore_numpy_warnings, default_internal
 from ..utils.operations import (
     fft_convolve_torch,
     fft_convolve_multi_torch,
-    grid_integrate,
+    selective_integrate,
 )
-from ..utils.interpolate import _shift_Lanczos_kernel_torch, simpsons_kernel, curvature_kernel
+from ..utils.interpolate import _shift_Lanczos_kernel_torch
 from ._shared_methods import select_target
 from .. import AP_config
 
 __all__ = ["Component_Model"]
 
 
 class Component_Model(AutoPhot_Model):
@@ -34,18 +34,20 @@
     and model evaluation functions. This class also handles
     integration, PSF convolution, and computing the Jacobian matrix.
 
     Attributes:
       parameter_specs (dict): Specifications for the model parameters.
       _parameter_order (tuple): Fixed order of parameters.
       psf_mode (str): Technique and scope for PSF convolution.
-      sampling_mode (str): Method for initial sampling of model. Can be one of midpoint, trapezoid, simpson. Default: midpoint
-      sampling_tolerance (float): accuracy to which each pixel should be evaluated. Default: 1e-2
-      integrate_mode (str): Integration scope for the model. One of none, threshold, full where threshold will select which pixels to integrate while full (in development) will integrate all pixels. Default: threshold
-      softening (float): Softening length used for numerical stability and integration stability to avoid discontinuities (near R=0). Effectively has units of arcsec. Default: 1e-5
+      psf_window_size (int): Size in pixels of the PSF convolution box.
+      integrate_mode (str): Integration scope for the model.
+      integrate_window_size (int): Size of the window in which to perform integration.
+      integrate_factor (int): Factor by which to upscale each dimension when integrating.
+      integrate_recursion_factor (int): Relative size of windows between recursion levels.
+      integrate_recursion_depth (int): Number of recursion cycles to apply when integrating.
       jacobian_chunksize (int): Maximum size of parameter list before jacobian will be broken into smaller chunks.
       special_kwargs (list): Parameters which are treated specially by the model object and should not be updated directly.
       useable (bool): Indicates if the model is useable.
 
     Methods:
       initialize: Determine initial values for the center coordinates.
       sample: Evaluate the model on the space covered by an image object.
@@ -60,49 +62,44 @@
     # Fixed order of parameters for all methods that interact with the list of parameters
     _parameter_order = ("center",)
 
     # Scope for PSF convolution
     psf_mode = "none"  # none, full
     # Technique for PSF convolution
     psf_convolve_mode = "fft" # fft, direct
-    psf_subpixel_shift = True
-
-    # Method for initial sampling of model
-    sampling_mode = "midpoint" # midpoint, trapezoid, simpson
-    
-    # Level to which each pixel should be evaluated
-    sampling_tolerance = 1e-2
-    
+    # method for initial sampling of grid before subpixel integration
+    sampling_mode = "midpoint" # midpoint, trapezoid, simpson 
     # Integration scope for model
     integrate_mode = "threshold"  # none, threshold, full*
 
+    # Number of recursion cycles to apply when integrating (threshold mode)
+    integrate_recursion_depth = 3
+    # Threshold for triggering pixel integration (threshold mode)
+    integrate_threshold = 1e-2
+
     # Maximum size of parameter list before jacobian will be broken into smaller chunks, this is helpful for limiting the memory requirements to build a model, lower jacobian_chunksize is slower but uses less memory
     jacobian_chunksize = 10
 
-    # Softening length used for numerical stability and/or integration stability to avoid discontinuities (near R=0)
-    softening = 1e-5
-    
     # Parameters which are treated specially by the model object and should not be updated directly when initializing
     special_kwargs = ["parameters", "filename", "model_type"]
     track_attrs = [
         "psf_mode",
         "psf_convolve_mode",
         "sampling_mode",
-        "sampling_tolerance",
         "integrate_mode",
+        "integrate_recursion_depth",
+        "integrate_threshold",
         "jacobian_chunksize",
-        "softening",
     ]
     useable = False
 
     def __init__(self, name, *args, **kwargs):
         super().__init__(name, *args, **kwargs)
 
         self.psf = None
-        self.psf_aux_image = None
 
         # Set any user defined attributes for the model
         for kwarg in kwargs:  # fixme move to core model?
             # Skip parameters with special behaviour
             if kwarg in self.special_kwargs:
                 continue
             # Set the model parameter
@@ -116,45 +113,27 @@
         self.parameter_specs = self.build_parameter_specs(
             kwargs.get("parameters", None)
         )
         with torch.no_grad():
             self.build_parameters()
             if isinstance(kwargs.get("parameters", None), torch.Tensor):
                 self.parameters.set_values(kwargs["parameters"])
-                
-    def set_aux_psf(self, aux_psf, add_parameters = True):
-        """Set the PSF for this model as an auxiliary psf model. This psf
-        model will be resampled as part of the model sampling step to
-        track changes made during fitting.
-
-        Args:
-          aux_psf: The auxiliary psf model
-          add_parameters: if true, the parameters of the auxiliary psf model will become model parameters for this model as well. 
-
-        """
-        
-        self.psf = aux_psf
 
-        if add_parameters:
-            self.parameters.add_group(aux_psf.parameters)
-                            
     @property
     def psf(self):
         if self._psf is None:
             return self.target.psf
         return self._psf
 
     @psf.setter
     def psf(self, val):
         if val is None:
             self._psf = None
         elif isinstance(val, PSF_Image):
             self._psf = val
-        elif isinstance(val, AutoPhot_Model):
-            self._psf = val
         else:
             self._psf = PSF_Image(
                 val,
                 pixelscale=self.target.pixelscale,
                 band=self.target.band,
             )
 
@@ -233,15 +212,15 @@
           image (Image): The image defining the set of pixels on which to evaluate the model
 
         """
         if X is None or Y is None:
             Coords = image.get_coordinate_meshgrid()
             X, Y = Coords - parameters["center"].value[...,None, None]
         return torch.zeros_like(X)  # do nothing in base model
-                
+
     def sample(
         self,
         image: Optional["Image"] = None,
         window: Optional[Window] = None,
         parameters: Optional[Parameter_Group] = None,
     ):
         """Evaluate the model on the space covered by an image object. This
@@ -284,75 +263,122 @@
         if parameters is None:
             parameters = self.parameters
 
         if "window" in self.psf_mode:
             raise NotImplementedError("PSF convolution in sub-window not available yet")
 
         if "full" in self.psf_mode:
-            if isinstance(self.psf, AutoPhot_Model):
-                psf = self.psf.sample(image = self.psf_aux_image, parameters = parameters.groups[self.psf.name])
-                upscale = self.psf_aux_image.psf_upscale if self.psf_aux_image is not None else self.psf.psf_upscale
-                psf = PSF_Image(data = psf.data, pixelscale = psf.pixelscale, psf_upscale = upscale)
-            else:
-                psf = self.psf
             # Add border for psf convolution edge effects, will be cropped out later
-            working_window += psf.psf_border
+            working_window += self.psf.psf_border
             # Determine the pixels scale at which to evalaute, this is smaller if the PSF is upscaled
-            working_pixelscale = image.pixelscale / psf.psf_upscale
+            working_pixelscale = image.pixelscale / self.psf.psf_upscale
             # Make the image object to which the samples will be tracked
             working_image = Model_Image(
                 pixelscale=working_pixelscale, window=working_window
             )            
             # Sub pixel shift to align the model with the center of a pixel
-            if self.psf_subpixel_shift:
-                pixel_center = working_image.world_to_pixel(parameters["center"].value)
-                center_shift = pixel_center - torch.round(pixel_center) # torch.clamp(pixel_center - torch.round(pixel_center), -0.49, 0.49) # shifts smaller than 1/100th of a pixel are not allowed for numerical stability
-                working_image.header.pixel_shift_origin(center_shift)
-            else:
-                center_shift = None
+            pixel_center = working_image.world_to_pixel(parameters["center"].value)
+            center_shift = pixel_center - torch.round(pixel_center)
+            working_image.header.pixel_shift_origin(center_shift)
             # Evaluate the model at the current resolution
-            reference, deep = self._sample_init(
-                image=working_image, parameters=parameters, center = parameters["center"].value,
+            working_image.data += self.evaluate_model(
+                image=working_image, parameters=parameters
             )
             # If needed, super-resolve the image in areas of high curvature so pixels are properly sampled
-            deep = self._sample_integrate(deep, reference, working_image, parameters, parameters["center"].value)
-
-            # update the image with the integrated pixels
-            working_image.data += deep
-            
+            if self.integrate_mode == "none":
+                pass
+            elif self.integrate_mode == "threshold":
+                Coords = working_image.get_coordinate_meshgrid()
+                X, Y = Coords - parameters["center"].value[...,None, None]
+                selective_integrate(
+                    X=X,
+                    Y=Y,
+                    data=working_image.data,
+                    image_header=working_image.header,
+                    eval_brightness=self.evaluate_model,
+                    eval_parameters=parameters,
+                    max_depth=self.integrate_recursion_depth,
+                    integrate_threshold=self.integrate_threshold,
+                )
+            else:
+                raise ValueError(
+                    f"{self.name} has unknown integration mode: {self.integrate_mode}"
+                )
             # Convolve the PSF
-            self._sample_convolve(working_image, center_shift, psf)
+            pix_center_shift = working_image.world_to_pixel_delta(center_shift)
+            LL = _shift_Lanczos_kernel_torch(
+                -pix_center_shift[0],
+                -pix_center_shift[1],
+                3,
+                AP_config.ap_dtype,
+                AP_config.ap_device,
+            )
+            shift_psf = torch.nn.functional.conv2d(
+                self.psf.data.view(1, 1, *self.psf.data.shape),
+                LL.view(1, 1, *LL.shape),
+                padding="same",
+            ).squeeze()
+            # Remove unphysical negative pixels from Lanczos interpolation
+            shift_psf[shift_psf < 0] = torch.tensor(0., dtype=AP_config.ap_dtype, device = AP_config.ap_device)
+            if self.psf_convolve_mode == "fft":
+                working_image.data = fft_convolve_torch(
+                    working_image.data, shift_psf / torch.sum(shift_psf), img_prepadded=True
+                )
+            elif self.psf_convolve_mode == "direct":
+                working_image.data = torch.nn.functional.conv2d(
+                    working_image.data.view(1, 1, *working_image.data.shape),
+                    torch.flip(shift_psf.view(1, 1, *shift_psf.shape) / torch.sum(shift_psf), dims = (2,3)),
+                    padding="same",
+                ).squeeze()
+            else:
+                raise ValueError(f"unrecognized psf_convolve_mode: {self.psf_convolve_mode}")
                 
             # Shift image back to align with original pixel grid
-            if self.psf_subpixel_shift:
-                working_image.header.shift_origin(-center_shift)
+            working_image.header.shift_origin(-center_shift)
             # Add the sampled/integrated/convolved pixels to the requested image
-            working_image = working_image.reduce(psf.psf_upscale).crop(
-                psf.psf_border_int
+            working_image = working_image.reduce(self.psf.psf_upscale).crop(
+                self.psf.psf_border_int
             )
+            if self.mask is not None:
+                working_image.data = working_image.data * torch.logical_not(self.mask)
+            image += working_image
 
         else:
-            
             # Create an image to store pixel samples
             working_image = Model_Image(
                 pixelscale=image.pixelscale, window=working_window
             )
             # Evaluate the model on the image
-            reference, deep = self._sample_init(
-                image=working_image, parameters=parameters, center = parameters["center"].value,
+            working_image.data += self.evaluate_model(
+                image=working_image, parameters=parameters
             )
             # Super-resolve and integrate where needed
-            deep = self._sample_integrate(deep, reference, working_image, parameters, center = parameters["center"].value)
+            if self.integrate_mode == "none":
+                pass
+            elif self.integrate_mode == "threshold":
+                Coords = working_image.get_coordinate_meshgrid()
+                X, Y = Coords - parameters["center"].value[...,None, None]
+                selective_integrate(
+                    X=X,
+                    Y=Y,
+                    data=working_image.data,
+                    image_header=working_image.header,
+                    eval_brightness=self.evaluate_model,
+                    eval_parameters=parameters,
+                    max_depth=self.integrate_recursion_depth,
+                    integrate_threshold=self.integrate_threshold,
+                )
+            else:
+                raise ValueError(
+                    f"{self.name} has unknown integration mode: {self.integrate_mode}"
+                )
             # Add the sampled/integrated pixels to the requested image
-            working_image.data += deep
-            
-        if self.mask is not None:
-            working_image.data = working_image.data * torch.logical_not(self.mask)
-                
-        image += working_image
+            if self.mask is not None:
+                working_image.data = working_image.data * torch.logical_not(self.mask)
+            image += working_image
 
         return image
 
     @torch.no_grad()
     def jacobian(
         self,
         parameters: Optional[torch.Tensor] = None,
@@ -545,12 +571,9 @@
         for P in state["parameter_order"]:
             self.parameters.add_parameter(Parameter(**state["parameters"][P]))
         self.parameters.to(dtype=AP_config.ap_dtype, device=AP_config.ap_device)
         return state
 
     # Extra background methods for the basemodel
     ######################################################################
-    from ._model_methods import _sample_init
-    from ._model_methods import _sample_integrate
-    from ._model_methods import _sample_convolve
     from ._model_methods import build_parameter_specs
     from ._model_methods import build_parameters
```

## autophot/models/parameter_group.py

```diff
@@ -295,34 +295,29 @@
                 as_representation=as_representation,
             )
             start += V
 
     def __iter__(self):
         return filter(lambda p: not p.locked, self.parameters.values())
 
-    def iter_all(self):
-        return self.parameters.values()
-        
     def get_id(self, key):
         if ":" in key:
             return self.parameters[key[: key.find(":")]]
         else:
             return self.parameters[key]
 
     def get_name(self, key):
-        # The : character is used for nested parameter group names
         if ":" in key:
             return self.groups[key[: key.find(":")]].get_name(key[key.find(":") + 1 :])
-
-        # Attempt to find the parameter with that key name
-        for P in self.parameters.values():
-            if P.name == key:
-                return P
-        # If the key cannot be found, raise an error
-        raise KeyError()
+        else:
+            for P in self.parameters.values():
+                if P.name == key:
+                    return P
+            else:
+                raise KeyError()
 
     def pop_id(self, key):
         try:
             del self.parameters[key]
         except KeyError:
             pass
         for group in self.groups.values():
@@ -353,10 +348,7 @@
             self.get_name(key)
             return True
         except KeyError:
             return False
 
     def __len__(self):
         return len(self.parameters)
-
-    def __str__(self):
-        return f"Parameter Group: {self.name}\n" + "\n".join(list(str(P) for P in self.iter_all()))
```

## autophot/models/parameter_object.py

```diff
@@ -486,24 +486,15 @@
         self.prof = state.get("prof", None)
 
     def __str__(self):
         """String representation of the parameter which indicates it's value
         along with uncertainty, units, limits, etc.
 
         """
-
-        value = self.value.detach().cpu().tolist() if self.value is not None else "None"
-        uncertainty = self.uncertainty.detach().cpu().tolist() if self.uncertainty is not None else "None"
-        if self.limits is None:
-            limits = None
-        else:
-            limits0 = self.limits[0].detach().cpu().tolist() if self.limits[0] is not None else "None"
-            limits1 = self.limits[1].detach().cpu().tolist() if self.limits[1] is not None else "None"
-            limits = (limits0,limits1)
-        return f"{self.name}: {value} +- {uncertainty} [{self.units}{'' if self.locked is False else ', locked'}{'' if limits is None else (', ' + str(limits))}{'' if self.cyclic is False else ', cyclic'}]"
+        return f"{self.name}: {self.value} +- {self.uncertainty} [{self.units}{'' if self.locked is False else ', locked'}{'' if self.limits is None else (', ' + str(self.limits))}{'' if self.cyclic is False else ', cyclic'}]"
 
     def __iter__(self):
         """If the parameter has multiple values, it is posible to iterate over
         the values.
 
         """
         self.i = -1
```

## autophot/models/psf_model.py

```diff
@@ -1,59 +1,55 @@
 import torch
 
 from .star_model_object import Star_Model
 from ..image import Model_Image
 from ..utils.decorators import ignore_numpy_warnings, default_internal
-from ..utils.interpolate import interp2d
+from ..utils.interpolate import _shift_Lanczos_kernel_torch
 from ._shared_methods import select_target
 from .. import AP_config
 
 __all__ = ["PSF_Star"]
 
 
 class PSF_Star(Star_Model):
     """Star model which uses an image of the PSF as it's representation
-    for stars. Using bilinear interpolation it will shift the PSF
+    for stars. Using Lanczos interpolation it will shift the PSF
     within a pixel to accurately represent the center location of a
     point source. There is no funcitonal form for this object type as
     any image can be supplied. Note that as an argument to the model
     at construction one can provide "psf" as an AutoPhot Model_Image
-    object. Since only bilinear interpolation is performed, it is
-    recommended to provide the PSF at a higher resolution than the
-    image if it is near the nyquist sampling limit. Bilinear
-    interpolation is very fast and accurate for smooth models, so this
-    way it is possible to do the expensive interpolation before
-    optimization and save time. Note that if you do this you must
-    provide the PSF as a Model_Image object with the correct PSF
-    (essentially just divide the PSF by the upsampling factor you
-    used).
+    object. If the supplied image is at a higher resolution than the
+    target image then the PSF will be upsampled at the time of
+    sampling after the shift has been performed. In this way it is
+    possible to get a more accurate representation of the PSF.
 
     Parameters:
         flux: the total flux of the star model, represented as the log of the total flux.
-
     """
 
     model_type = f"psf {Star_Model.model_type}"
     parameter_specs = {
         "flux": {"units": "log10(flux/arcsec^2)"},
     }
     _parameter_order = Star_Model._parameter_order + ("flux",)
     useable = True
 
+    lanczos_kernel_size = 5
+    clip_lanczos_kernel = True
+
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         # fixme, model already has PSF interface, those can just be merged
         if "psf" in kwargs:
             self.psf_model = kwargs["psf"]
         else:
             self.psf_model = Model_Image(
                 data=torch.clone(self.psf.data),
                 pixelscale=self.psf.pixelscale,
             )
-        self.psf_model.header.shift_origin(self.psf_model.origin - self.psf_model.center)
 
     @torch.no_grad()
     @ignore_numpy_warnings
     @select_target
     @default_internal
     def initialize(self, target=None, parameters=None, **kwargs):
         super().initialize(target=target, parameters=parameters)
@@ -68,29 +64,36 @@
         if parameters["flux"].uncertainty is None:
             parameters["flux"].set_uncertainty(
                 torch.abs(parameters["flux"].value) * 1e-2, override_locked=True
             )
 
     @default_internal
     def evaluate_model(self, X=None, Y=None, image=None, parameters=None, **kwargs):
-        if X is None:
-            Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[...,None, None]
-
-        # Convert coordinates into pixel locations in the psf image
-        pX, pY = self.psf_model.world_to_pixel(torch.stack((X, Y)).view(2,-1), unsqueeze_origin = True)
-        pX = pX.reshape(X.shape)
-        pY = pY.reshape(Y.shape)
-
-        # Select only the pixels where the PSF image is defined
-        select = torch.logical_and(
-            torch.logical_and(pX > -0.5, pX < self.psf_model.data.shape[1]),
-            torch.logical_and(pY > -0.5, pY < self.psf_model.data.shape[0]),
+        new_origin = parameters["center"].value - self.psf_model.window.end / 2
+        pixel_origin = image.pixel_to_world(torch.round(image.world_to_pixel(new_origin)))
+        pixel_shift = (
+            image.world_to_pixel(new_origin) - image.world_to_pixel(pixel_origin)
+        )
+        LL = _shift_Lanczos_kernel_torch(
+            -pixel_shift[0],
+            -pixel_shift[1],
+            3,
+            AP_config.ap_dtype,
+            AP_config.ap_device,
+        )
+        psf = Model_Image(
+            data=torch.nn.functional.conv2d(
+                (
+                    torch.clone(self.psf_model.data)
+                    * ((10 ** parameters["flux"].value) * image.pixel_area)
+                ).view(1, 1, *self.psf_model.data.shape),
+                LL.view(1, 1, *LL.shape),
+                padding="same",
+            )[0][0],
+            origin=new_origin,
+            pixelscale=self.psf_model.pixelscale,
         )
 
-        # Zero everywhere outside the psf
-        result = torch.zeros_like(X)
-
-        # Use bilinear interpolation of the PSF at the requested coordinates
-        result[select] = interp2d(self.psf_model.data, pX[select], pY[select])
-
-        return result * ((10**parameters["flux"].value) * image.pixel_area)
+        # fixme pick nearest neighbor for each X, Y? interpolate?
+        img = image.blank_copy()
+        img += psf
+        return img.data
```

## autophot/models/star_model_object.py

```diff
@@ -20,16 +20,16 @@
 
     model_type = f"star {Component_Model.model_type}"
     useable = False
 
     @default_internal
     def radius_metric(self, X, Y, image=None, parameters=None):
         return torch.sqrt(
-            (torch.abs(X)) ** 2 + (torch.abs(Y)) ** 2
-        )
+            (torch.abs(X) + 1e-8) ** 2 + (torch.abs(Y) + 1e-8) ** 2
+        )  # epsilon added for numerical stability of gradient
 
     @property
     def psf_mode(self):
         return "none"
 
     @psf_mode.setter
     def psf_mode(self, val):
```

## autophot/models/superellipse_model.py

```diff
@@ -34,16 +34,16 @@
     }
     _parameter_order = Galaxy_Model._parameter_order + ("C0",)
     useable = False
 
     @default_internal
     def radius_metric(self, X, Y, image=None, parameters=None):
         return torch.pow(
-            torch.pow(torch.abs(X), parameters["C0"].value + 2.0)
-            + torch.pow(torch.abs(Y), parameters["C0"].value + 2.0),
+            torch.pow(torch.abs(X) + 1e-8, parameters["C0"].value + 2.0)
+            + torch.pow(torch.abs(Y) + 1e-8, parameters["C0"].value + 2.0),
             1.0 / (parameters["C0"].value + 2.0),
         )  # epsilon added for numerical stability of gradient
 
 
 class SuperEllipse_Warp(Warp_Galaxy):
     """Expanded warp model which includes a superellipse transformation
     in its radius metric. This allows for the expression of "boxy" and
@@ -71,11 +71,11 @@
     }
     _parameter_order = Warp_Galaxy._parameter_order + ("C0",)
     useable = False
 
     @default_internal
     def radius_metric(self, X, Y, image=None, parameters=None):
         return torch.pow(
-            torch.pow(torch.abs(X), parameters["C0"].value + 2.0)
-            + torch.pow(torch.abs(Y), parameters["C0"].value + 2.0),
+            torch.pow(torch.abs(X) + 1e-8, parameters["C0"].value + 2.0)
+            + torch.pow(torch.abs(Y) + 1e-8, parameters["C0"].value + 2.0),
             1.0 / (parameters["C0"].value + 2.0),
         )  # epsilon added for numerical stability of gradient
```

## autophot/plots/image.py

```diff
@@ -13,34 +13,14 @@
 from .visuals import *
 
 
 __all__ = ["target_image", "model_image", "residual_image", "model_window"]
 
 
 def target_image(fig, ax, target, window=None, **kwargs):
-    """
-    This function is used to display a target image using the provided figure and axes.
-
-    Args:
-        fig (matplotlib.figure.Figure): The figure object in which the target image will be displayed.
-        ax (matplotlib.axes.Axes): The axes object on which the target image will be plotted.
-        target (Image or Image_List): The image or list of images to be displayed. 
-        window (Window, optional): The window through which the image is viewed. If `None`, the window of the 
-            provided `target` is used. Defaults to `None`.
-        **kwargs: Arbitrary keyword arguments.
-
-    Returns:
-        fig (matplotlib.figure.Figure): The figure object containing the displayed target image.
-        ax (matplotlib.axes.Axes): The axes object containing the displayed target image.
-
-    Note:
-        If the `target` is an `Image_List`, this function will recursively call itself for each image in the list. 
-        The `window` parameter and `kwargs` are passed unchanged to each recursive call.
-    """
-    
     # recursive call for target image list
     if isinstance(target, Image_List):
         for i in range(len(target.image_list)):
             target_image(fig, ax[i], target.image_list[i], window=window, **kwargs)
         return fig, ax
     if window is None:
         window = target.window
@@ -86,110 +66,56 @@
     ax,
     model,
     sample_image=None,
     window=None,
     target=None,
     showcbar=True,
     target_mask=False,
-    cmap_levels=None,
     **kwargs,
 ):
-    """
-    This function is used to generate a model image and display it using the provided figure and axes.
-
-    Args:
-        fig (matplotlib.figure.Figure): The figure object in which the image will be displayed.
-        ax (matplotlib.axes.Axes): The axes object on which the image will be plotted.
-        model (Model): The model object used to generate a model image if `sample_image` is not provided.
-        sample_image (Image or Image_List, optional): The image or list of images to be displayed. 
-            If `None`, a model image is generated using the provided `model`. Defaults to `None`.
-        window (Window, optional): The window through which the image is viewed. If `None`, the window of the 
-            provided `model` is used. Defaults to `None`.
-        target (Target, optional): The target or list of targets for the image or image list. 
-            If `None`, the target of the `model` is used. Defaults to `None`.
-        showcbar (bool, optional): Whether to show the color bar. Defaults to `True`.
-        target_mask (bool, optional): Whether to apply the mask of the target. If `True` and if the target has a mask,
-            the mask is applied to the image. Defaults to `False`.
-        cmap_levels (int, optional): The number of discrete levels to convert the continuous color map to. 
-            If not `None`, the color map is converted to a ListedColormap with the specified number of levels. 
-            Defaults to `None`.
-        **kwargs: Arbitrary keyword arguments. These are used to override the default imshow_kwargs.
-
-    Returns:
-        fig (matplotlib.figure.Figure): The figure object containing the displayed image.
-        ax (matplotlib.axes.Axes): The axes object containing the displayed image.
-
-    Note:
-        If the `sample_image` is an `Image_List`, this function will recursively call itself for each image in the list,
-        with the corresponding target and window. The `showcbar` parameter and `kwargs` are passed unchanged to each recursive call.
-    """
-    
     if sample_image is None:
         sample_image = model.make_model_image()
         sample_image = model(sample_image)
-
-    # Use model target if not given
     if target is None:
         target = model.target
-
-    # Use model window if not given
     if window is None:
         window = model.window
-
-    # Handle image lists
     if isinstance(sample_image, Image_List):
         for i, images in enumerate(zip(sample_image, target, window)):
             model_image(
                 fig,
                 ax[i],
                 model,
                 sample_image=images[0],
                 window=images[2],
                 target=images[1],
                 showcbar=showcbar,
                 **kwargs,
             )
         return fig, ax
 
-    # Evaluate the model image
     X, Y = sample_image.get_coordinate_corner_meshgrid()
     sample_image = sample_image.data.detach().cpu().numpy()
-
-    # Default kwargs for image
     imshow_kwargs = {
         "cmap": cmap_grad,
         "norm": matplotlib.colors.LogNorm(),  # "norm": ImageNormalize(stretch=LogStretch(), clip=False),
     }
-
-    # Update with user provided kwargs
     imshow_kwargs.update(kwargs)
-    
-    # if requested, convert the continuous colourmap into discrete levels
-    if cmap_levels is not None:
-        imshow_kwargs["cmap"] = matplotlib.colors.ListedColormap(list(imshow_kwargs["cmap"](c) for c in np.linspace(0.,1.,cmap_levels)))
-        
-    # If zeropoint is available, convert to surface brightness units
     if target.zeropoint is not None:
         sample_image = flux_to_sb(
             sample_image, target.pixel_area.item(), target.zeropoint.item()
         )
         del imshow_kwargs["norm"]
         imshow_kwargs["cmap"] = imshow_kwargs["cmap"].reversed()
 
-    # Apply the mask if available
     if target_mask and target.has_mask:
         sample_image[target.mask.detach().cpu().numpy()] = np.nan
 
-    # Plot the image
     im = ax.pcolormesh(X, Y, sample_image, **imshow_kwargs)
-
-    # Enforce equal spacing on x y
     ax.axis("equal")
-
-    # Add a colourbar
     if showcbar:
         if target.zeropoint is not None:
             clb = fig.colorbar(im, ax=ax, label="Surface Brightness [mag/arcsec$^2$]")
             clb.ax.invert_yaxis()
         else:
             clb = fig.colorbar(im, ax=ax, label=f"log$_{{10}}$(flux)")
 
@@ -206,47 +132,14 @@
     showcbar=True,
     window=None,
     center_residuals=False,
     clb_label=None,
     normalize_residuals=False,
     **kwargs,
 ):
-    """
-    This function is used to calculate and display the residuals of a model image with respect to a target image.
-    The residuals are calculated as the difference between the target image and the sample image.
-
-    Args:
-        fig (matplotlib.figure.Figure): The figure object in which the residuals will be displayed.
-        ax (matplotlib.axes.Axes): The axes object on which the residuals will be plotted.
-        model (Model): The model object used to generate a model image if `sample_image` is not provided.
-        target (Target or Image_List, optional): The target or list of targets for the image or image list. 
-            If `None`, the target of the `model` is used. Defaults to `None`.
-        sample_image (Image or Image_List, optional): The image or list of images from which residuals will be calculated.
-            If `None`, a model image is generated using the provided `model`. Defaults to `None`.
-        showcbar (bool, optional): Whether to show the color bar. Defaults to `True`.
-        window (Window or Window_List, optional): The window through which the image is viewed. If `None`, the window of the 
-            provided `model` is used. Defaults to `None`.
-        center_residuals (bool, optional): Whether to subtract the median of the residuals. If `True`, the median is subtracted 
-            from the residuals. Defaults to `False`.
-        clb_label (str, optional): The label for the colorbar. If `None`, a default label is used based on the normalization of the 
-            residuals. Defaults to `None`.
-        normalize_residuals (bool, optional): Whether to normalize the residuals. If `True`, residuals are divided by the square root 
-            of the variance of the target. Defaults to `False`.
-        **kwargs: Arbitrary keyword arguments. These are used to override the default imshow_kwargs.
-
-    Returns:
-        fig (matplotlib.figure.Figure): The figure object containing the displayed residuals.
-        ax (matplotlib.axes.Axes): The axes object containing the displayed residuals.
-
-    Note:
-        If the `window`, `target`, or `sample_image` are lists, this function will recursively call itself for each element in the list,
-        with the corresponding window, target, and sample image. The `showcbar`, `center_residuals`, and `kwargs` are passed unchanged to 
-        each recursive call.
-    """
-    
     if window is None:
         window = model.window
     if target is None:
         target = model.target
     if sample_image is None:
         sample_image = model.make_model_image()
         sample_image = model(sample_image)
```

## autophot/utils/interpolate.py

```diff
@@ -1,15 +1,12 @@
-from functools import lru_cache
-
 import numpy as np
 import torch
 import matplotlib.pyplot as plt
 from astropy.convolution import convolve, convolve_fft
 from torch.nn.functional import conv2d
-
 from .operations import fft_convolve_torch
 
 
 def _h_poly(t):
     """Helper function to compute the 'h' polynomial matrix used in the
     cubic spline.
 
@@ -294,76 +291,7 @@
     return np.array(flux)
 
 
 def interp1d_torch(x_in, y_in, x_out):
     indices = torch.searchsorted(x_in[:-1], x_out) - 1
     weights = (y_in[1:] - y_in[:-1]) / (x_in[1:] - x_in[:-1])
     return y_in[indices] + weights[indices] * (x_out - x_in[indices])
-
-def interp2d(
-    im: torch.Tensor,
-    x: torch.Tensor,
-    y: torch.Tensor,
-) -> torch.Tensor:
-    """
-    Interpolates a 2D image at specified coordinates.
-    Similar to `torch.nn.functional.grid_sample` with `align_corners=False`.
-
-    Args:
-        im (Tensor): A 2D tensor representing the image.
-        x (Tensor): A tensor of x coordinates (in pixel space) at which to interpolate.
-        y (Tensor): A tensor of y coordinates (in pixel space) at which to interpolate.
-    
-    Returns:
-        Tensor: Tensor with the same shape as `x` and `y` containing the interpolated values.
-    """
-
-    # Convert coordinates to pixel indices
-    h, w = im.shape
-
-    # reshape for indexing purposes
-    start_shape = x.shape
-    x = x.view(-1)
-    y = y.view(-1)
-    
-    x0 = x.floor().long()
-    y0 = y.floor().long()
-    x1 = x0 + 1
-    y1 = y0 + 1
-    x0 = x0.clamp(0, w - 2)
-    x1 = x1.clamp(1, w - 1)
-    y0 = y0.clamp(0, h - 2)
-    y1 = y1.clamp(1, h - 1)
-    
-    fa = im[y0, x0]
-    fb = im[y1, x0]
-    fc = im[y0, x1]
-    fd = im[y1, x1]
-    
-    wa = (x1 - x) * (y1 - y)
-    wb = (x1 - x) * (y - y0)
-    wc = (x - x0) * (y1 - y)
-    wd = (x - x0) * (y - y0)
-
-    result = fa * wa + fb * wb + fc * wc + fd * wd
-
-    return result.view(*start_shape)
-
-@lru_cache(maxsize=32)
-def curvature_kernel(dtype, device):
-    kernel = torch.tensor(
-        [[0., 1.0, 0.], [1.0, -4, 1.0], [0.0, 1.0, 0.0]], #[[1., -2.0, 1.], [-2.0, 4, -2.0], [1.0, -2.0, 1.0]],
-        device=device,
-        dtype=dtype,
-    ) / 8
-    return kernel
-    
-@lru_cache(maxsize=32)
-def simpsons_kernel(dtype, device):
-    kernel = torch.ones(1,1,3,3, dtype = dtype, device = device)
-    kernel[0,0,1,1] = 16.
-    kernel[0,0,1,0] = 4.
-    kernel[0,0,0,1] = 4.
-    kernel[0,0,1,2] = 4.
-    kernel[0,0,2,1] = 4.
-    kernel = kernel / 36.
-    return kernel
```

## autophot/utils/operations.py

```diff
@@ -1,16 +1,15 @@
-from functools import lru_cache
 from typing import Callable, Optional
 
 import torch
 import matplotlib.pyplot as plt
 import numpy as np
 from astropy.convolution import convolve, convolve_fft
 from scipy.fft import next_fast_len
-from scipy.special import roots_legendre
+
 
 def fft_convolve_torch(img, psf, psf_fft=False, img_prepadded=False):
     # Ensure everything is tensor
     img = torch.as_tensor(img)
     psf = torch.as_tensor(psf)
 
     if img_prepadded:
@@ -86,72 +85,104 @@
 def displacement_grid(Nx, Ny, pixelscale=None, dtype=torch.float64, device="cpu"):
     px = displacement_spacing(Nx, dtype=dtype, device=device)
     py = displacement_spacing(Ny, dtype=dtype, device=device)
     PX, PY = torch.meshgrid(px, py, indexing = "xy")
     return (pixelscale @ torch.stack((PX, PY)).view(2,-1)).reshape((2, *PX.shape))
 
 
-@lru_cache(maxsize=32)
-def quad_table(n, p, dtype, device):
-    """
-    from: https://pomax.github.io/bezierinfo/legendre-gauss.html
-    """
-    abscissa, weights = roots_legendre(n)
+def selective_integrate(
+    X: torch.Tensor,
+    Y: torch.Tensor,
+    data: torch.Tensor,
+    image_header: "Image_Header",
+    eval_brightness: Callable,
+    eval_parameters: "Parameter_Group",
+    max_depth: int = 3,
+    _depth: int = 1,
+    _reference_brightness: Optional[float] = None,
+    integrate_threshold: float = 1e-2,
+):
+    """Sample the model at higher resolution than the input image.
 
-    w = torch.tensor(weights, dtype = dtype, device = device)
-    a = torch.tensor(abscissa, dtype = dtype, device = device)
-    X, Y = torch.meshgrid(a, a, indexing = "xy")
-
-    W = torch.outer(w,w) / 4.
-
-    X, Y = p @ (torch.stack((X, Y)).view(2,-1) / 2.)
-    
-    return X, Y, W.reshape(-1) 
-
-def single_quad_integrate(X, Y, image_header, eval_brightness, eval_parameters, dtype, device, quad_level = 3):
-    
-    # collect gaussian quadrature weights
-    abscissaX, abscissaY, weight = quad_table(quad_level, image_header.pixelscale, dtype, device)
-    
-    # Specify coordinates at which to evaluate function
-    Xs = torch.repeat_interleave(X[...,None], quad_level**2, -1) + abscissaX
-    Ys = torch.repeat_interleave(Y[...,None], quad_level**2, -1) + abscissaY
+    This function selectively refines the integration of an input
+    image based on the local curvature of the image data.  It
+    recursively evaluates the model at higher resolutions in areas
+    where the curvature exceeds the specified threshold.  With
+    each level of recursion, the function refines the affected
+    areas using a 3x3 grid for super-resolution.
+
+    Args:
+      X (torch.tensor): A tensor representing the X coordinates of the input image.
+      Y (torch.tensor): A tensor representing the Y coordinates of the input image.
+      data (torch.tensor): A tensor containing the input image data.
+      image_header (Image_Header): An instance of the Image_Header class containing the image's header information.
+      eval_brightness (Callable): Function which evaluates the brightness at a given coordinate.
+      _depth (int, optional): The current recursion depth. Default is 1.
+      max_depth (int, optional): The maximum recursion depth allowed. Default is 3.
+      _reference_brightness (float or None, optional): The reference brightness value used to normalize the curvature
+                                                       values. If None, the maximum value of the input data divided by
+                                                       10 will be used. Default is None.
 
-    # Evaluate the model at the quadrature points
+    Returns:
+        None. The function updates the input data tensor in-place with the selectively integrated values.
+
+    """
+    # check recursion depth, exit if too deep
+    if _depth > max_depth:
+        return
+
+    with torch.no_grad():
+        if _reference_brightness is None:
+            _reference_brightness = torch.max(data) / 10
+        curvature_kernel = torch.tensor(
+            [[0, 1.0, 0], [1.0, -4, 1.0], [0, 1.0, 0]],
+            device=data.device,
+            dtype=data.dtype,
+        )
+        if _depth == 1:
+            curvature = torch.abs(fft_convolve_torch(data, curvature_kernel))
+            curvature[:, 0] = 0
+            curvature[:, -1] = 0
+            curvature[0, :] = 0
+            curvature[-1, :] = 0
+            curvature /= _reference_brightness
+            select = curvature > integrate_threshold
+        else:
+            curvature = (
+                torch.sum(data * curvature_kernel, axis=(1, 2)) / _reference_brightness
+            )
+            select = curvature > integrate_threshold
+            select = select.view(-1, 1, 1).repeat(1, 3, 3)
+
+        # compute the subpixel coordinate shifts for even integration within a pixel
+        shiftsx, shiftsy = displacement_grid(
+            3,
+            3,
+            pixelscale=image_header.pixelscale,
+            device=data.device,
+            dtype=data.dtype,
+        )
+
+    # Reshape coordinates to add two dimensions with the super-resolved coordiantes
+    Xs = X[select].view(-1, 1, 1).repeat(1, 3, 3) + shiftsx
+    Ys = Y[select].view(-1, 1, 1).repeat(1, 3, 3) + shiftsy
+    # evaluate the model on the new smaller coordinate grid in each pixel
     res = eval_brightness(
-        X=Xs, Y=Ys, image=image_header, parameters=eval_parameters,
+        image=image_header.super_resolve(3), parameters=eval_parameters, X=Xs, Y=Ys
+    )
+
+    # Apply recursion to integrate any further pixels as needed
+    selective_integrate(
+        X=Xs,
+        Y=Ys,
+        data=res,
+        image_header=image_header.super_resolve(3),
+        eval_brightness=eval_brightness,
+        eval_parameters=eval_parameters,
+        _depth=_depth + 1,
+        max_depth=max_depth,
+        _reference_brightness=_reference_brightness,
+        integrate_threshold=integrate_threshold,
     )
 
-    ref = res.mean(axis = -1)
-    # Apply the weights and reduce to original pixel space
-    res = (res*weight).sum(axis=-1)
-    
-    return res, ref
-        
-def grid_integrate(X, Y, value, compare, image_header, eval_brightness, eval_parameters, dtype, device, tolerance = 1e-2, quad_level = 3, gridding = 5, grid_level = 0, max_level = 2, reference = None):
-    if grid_level >= max_level:
-        return value
-
-    # Evaluate gaussian quadrature on the specified pixels
-    res, ref = single_quad_integrate(X, Y, image_header, eval_brightness, eval_parameters, dtype, device, quad_level = quad_level)
-
-    # Determine which pixels are now converged to sufficient degree
-    error = torch.abs((res - ref))
-    select = error > (tolerance*reference)
-
-    # Update converged pixels with new value
-    value[torch.logical_not(select)] = res[torch.logical_not(select)]
-
-    # Set up sub-gridding to super resolve problem pixels
-    stepx, stepy = displacement_grid(gridding,gridding,image_header.pixelscale, dtype, device)
-    # Write out the coordinates for the super resolved pixels
-    Xs = torch.repeat_interleave(X[select][...,None], gridding**2, -1) + stepx.reshape(-1)
-    Ys = torch.repeat_interleave(Y[select][...,None], gridding**2, -1) + stepy.reshape(-1)
-    # Copy the current pixel values into new shape with super resolved pixels
-    deep_res = torch.repeat_interleave(res[select][...,None], gridding**2, -1)
-
-    # Recursively evaluate the pixels at the higher gridding
-    deep_res = grid_integrate(Xs, Ys, deep_res/gridding**2, None, image_header.super_resolve(gridding), eval_brightness, eval_parameters, dtype, device, tolerance, quad_level + 1, gridding, grid_level + 1, max_level, reference = reference*gridding**2)
-
-    # Update the pixels that have been sub-integrated
-    value[select] = deep_res.sum(axis=(-1,))
-    return value
+    # Update the pixels with the new integrated values
+    data[select] = res.sum(axis=(1, 2))
```

## autophot/utils/optimization.py

```diff
@@ -1,9 +1,8 @@
 import torch
-
 from .. import AP_config
 
 
 def chi_squared(target, model, mask=None, variance=None):
     if mask is None:
         if variance is None:
             return torch.sum((target - model) ** 2)
```

## autophot/utils/parametric_profiles.py

```diff
@@ -12,16 +12,16 @@
         R: Radii tensor at which to evaluate the sersic function
         n: sersic index restricted to n > 0.36
         Re: Effective radius in the same units as R
         Ie: Effective surface density
     """
     bn = sersic_n_to_b(n)
     return Ie * torch.exp(
-        -bn * (torch.pow(R / Re, 1 / n) - 1)
-    ) 
+        -bn * (torch.pow((R + 1e-8) / Re, 1 / n) - 1)
+    )  # epsilon added for numerical stability of gradient
 
 
 def sersic_np(R, n, Re, Ie):
     """Sersic 1d profile function, works more generally with numpy
     operations. In the event that impossible values are passed to the
     function it returns large values to guide optimizers away from
     such values.
@@ -127,19 +127,20 @@
         Ib: brightness at the scale length, represented as the log of the brightness divided by pixel scale squared.
         Rb: scale length radius
         alpha: sharpness of transition between power law slopes
         beta: outer power law slope
         gamma: inner power law slope
 
     """
+    Rplus = R + 1e-8  # added for numerical stability near R = 0
     return (
         Ib
         * (2 ** ((beta - gamma) / alpha))
-        * ((R / Rb) ** (-gamma))
-        * ((1 + (R / Rb) ** alpha) ** ((gamma - beta) / alpha))
+        * ((Rplus / Rb) ** (-gamma))
+        * ((1 + (Rplus / Rb) ** alpha) ** ((gamma - beta) / alpha))
     )
 
 
 def nuker_np(R, Rb, Ib, alpha, beta, gamma):
     """Nuker 1d profile function, works with numpy functions
 
     Parameters:
```

## Comparing `autophot-0.10.0.dist-info/LICENSE` & `autophot-0.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `autophot-0.10.0.dist-info/METADATA` & `autophot-0.9.0.dist-info/METADATA`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: autophot
-Version: 0.10.0
+Version: 0.9.0
 Summary: A fast, flexible, differentiable, and automated astronomical image modelling tool for precise parallel multi-wavelength photometry
 Home-page: https://github.com/Autostronomy/AutoPhot
 Author: Connor Stone
 Author-email: connorstone628@gmail.com
 License: GPL-3.0 license
 Classifier: Development Status :: 1 - Planning
 Classifier: Intended Audience :: Science/Research
@@ -27,15 +27,15 @@
   <source media="(prefers-color-scheme: dark)" srcset="media/AP_logo_white.png">
   <source media="(prefers-color-scheme: light)" srcset="media/AP_logo.png">
   <img alt="AutoPhot logo" src="media/AP_logo.png" width="70%">
 </picture>
 
 
 [![unittests](https://github.com/Autostronomy/AutoPhot/actions/workflows/testing.yaml/badge.svg?branch=main)](https://github.com/Autostronomy/AutoPhot/actions/workflows/testing.yaml)
-[![docs](https://github.com/Autostronomy/AutoPhot/actions/workflows/documentation.yaml/badge.svg?branch=main)](https://autostronomy.github.io/AutoPhot/)
+[![docs](https://github.com/Autostronomy/AutoPhot/actions/workflows/documentation.yaml/badge.svg?branch=main)](https://connorstoneastro.github.io/AutoPhot/)
 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
 [![pypi](https://img.shields.io/pypi/v/autophot.svg?logo=pypi&logoColor=white&label=PyPI)](https://pypi.org/project/autophot/)
 [![downloads](https://img.shields.io/pypi/dm/autophot?label=PyPI%20Downloads)](https://libraries.io/pypi/autophot)
 [![codecov](https://img.shields.io/codecov/c/github/Autostronomy/AutoPhot?logo=codecov)](https://app.codecov.io/gh/Autostronomy/AutoPhot?search=&displayType=list)
 
 AutoPhot is a fast, flexible, and automated astronomical image modelling tool for precise parallel multi-wavelength photometry. It is a python based package that uses PyTorch to quickly and efficiently perform analysis tasks. Written by [Connor Stone](https://connorjstone.com/) for tasks such as LSB imaging, handling crowded fields, multi-band photometry, and analyzing massive data from future telescopes. AutoPhot is flexible and fast for any astronomical image modelling task. While it uses PyTorch (originally developed for Machine Learning) it is NOT a machine learning based tool.
 
@@ -47,16 +47,16 @@
 pip install autophot
 ```
 
 If PyTorch gives you any trouble on your system, just follow the instructions on the [pytorch website](https://pytorch.org/) to install a version for your system.
 
 Also note that AutoPhot is only available for python3.
 
-See [the documentation](https://autostronomy.github.io/AutoPhot/) for more details.
+See [the documentation](https://connorstoneastro.github.io/AutoPhot/) for more details.
 
 ## Documentation
 
-You can find the documentation at the [GitHub Pages site connected with the AutoPhot project](https://autostronomy.github.io/AutoPhot/) which covers many of the main use cases for AutoPhot. It is still in development, but lots of useful information is there. Feel free to contact the author, [Connor Stone](https://connorjstone.com/), for any questions not answered by the documentation or tutorials.
+You can find the documentation at the [GitHub Pages site connected with the AutoPhot project](https://connorstoneastro.github.io/AutoPhot/) which covers many of the main use cases for AutoPhot. It is still in development, but lots of useful information is there. Feel free to contact the author, [Connor Stone](https://connorjstone.com/), for any questions not answered by the documentation or tutorials.
 
 ## Credit / Citation
 
-If you use AutoPhot in your research, please follow the [citation instructions here](https://autostronomy.github.io/AutoPhot/citation.html). A new paper for the updated AutoPhot code is in the works.
+If you use AutoPhot in your research, please follow the [citation instructions here](https://connorstoneastro.github.io/AutoPhot/citation.html). A new paper for the updated AutoPhot code is in the works.
```

## Comparing `autophot-0.10.0.dist-info/RECORD` & `autophot-0.9.0.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,69 +1,69 @@
 autophot/AP_config.py,sha256=554fT-07-OZFFxoi7-EDEvY90JSbC7zE7eos5Ilyf_Y,3171
-autophot/__init__.py,sha256=t4BNpXH4FlxABe9NKIsBA4XVyFA8G5opEoO9uh6MlMo,5720
+autophot/__init__.py,sha256=ychDtM1pOWWXxJEh3EPjokIkTwngdq6FwjinaT00Pk0,5719
 autophot/__main__.py,sha256=Ce_QoW2_-x3o3ZXbErfBFzJ_M20JzoaB3dUvwzAhXx4,281
 autophot/fit/__init__.py,sha256=lIizX8FXOMdBIRzmnL9dCbJZcOdnHSYw6e8Hgfay9Sk,1092
 autophot/fit/base.py,sha256=rluDW6eyCKhHLWSyHULizd74yPXd6LqoXrGUASMvQgs,6406
 autophot/fit/gp.py,sha256=PvMC6LeAIYWwDteVVo3AY7lb_TkQOY2-C5yWfK4CpUY,30
 autophot/fit/gradient.py,sha256=DyfzqK6I5LUV2LcdFHFDrr3C77WmMtxg9Pzf--2bZV0,6961
 autophot/fit/hmc.py,sha256=5uOtmmV6lz0ue06v6PTLVFALCbyfUlGR-dC1RCmcgX8,6939
 autophot/fit/iterative.py,sha256=vtbJYCXBkz9pfaud-kUIVIOlGZEWO09GStLGmoCyl44,13401
-autophot/fit/lm.py,sha256=w1H7IlvjvBcKIUDpl4xJsAmY5CbSy8dnCQzPKV0xEfU,28495
+autophot/fit/lm.py,sha256=T3pMHayI7FyEZ9frEmRP2rtnpcrRyxKKVA_PqiNKQeA,31419
 autophot/fit/mhmcmc.py,sha256=qOrvDuHQF2oteCXGy2yYhHRr3xqkvBSq6D2jWItTfWE,4381
 autophot/fit/nuts.py,sha256=o-i4mbfe1wGYFA-Zl79ORhMhrqJ9RWwvIB9pjMameZM,7150
 autophot/image/__init__.py,sha256=UPRYMGXs_WUIEkyoDh0467vU495wES6JOLkL9kFKMP8,1150
-autophot/image/image_header.py,sha256=qa-lfr9zzknNoKdCfo5DnKgjT6itx1Sqo1qRJFqGpWo,16996
-autophot/image/image_object.py,sha256=AAN1J9m_cyrYVMz-5svwdTdgN3t6EFFSF3QRWkdt9Pk,20693
+autophot/image/image_header.py,sha256=Khpcf203ERINVe4CETkdbkHg-agCFsb0cjD5HGJh5Uw,16790
+autophot/image/image_object.py,sha256=TlyOtyNBqJD0H6MIjvS6PvMS5AgXBUfyxgGjpy0RAxs,20545
 autophot/image/jacobian_image.py,sha256=1s3eAq3xB353i_srThMZVaJ7Wp-aXu0POLfvZ1JQSgg,5269
 autophot/image/model_image.py,sha256=geyEf-h3-8JHZY0ndT_iukaEpp5aS9uzacuKBKASa1M,6379
 autophot/image/psf_image.py,sha256=4fyUQ6PX7FVCs0vqf8kavu4I6SK4Rcp1d4Zenf26ltM,6791
 autophot/image/target_image.py,sha256=RR--6U6VULnGB3RnDEQuRuA39i-ECr-lOaJ_XLewO9M,14763
 autophot/image/window_object.py,sha256=FGMijIWaUW2bn67lcDZ_GVZfyHFfFt_ZgGexUIueGrs,23813
 autophot/models/__init__.py,sha256=V5ziTIawNay5GoqHIz-x1rnIOZqARe5VmMyFPCWLOR8,654
-autophot/models/_model_methods.py,sha256=hxs0NrbET2dVl43l1mFpH2b2luyukbXl_9be7L8OZF0,6441
-autophot/models/_shared_methods.py,sha256=lQyfbaqXTIRVBooA8p058PfRSD6bMlZIfidlRqGQlyM,19427
-autophot/models/core_model.py,sha256=scwRlwu9ZX9uQDBRBzIJmYNoBJYtM4vMyGBgGz3kOaI,12300
+autophot/models/_model_methods.py,sha256=ishhSsylSqw0YElOPDAiN8x-eL1-9_0RSxV_-M-irk0,1742
+autophot/models/_shared_methods.py,sha256=3OoOaBR2k4HBg6kto0ExfZMZk8B0XYfGSzztD0rV98Y,19240
+autophot/models/core_model.py,sha256=wBfrXDscC7GJYbc523ENyBlK6viLynY8Uih-snXQwG0,12502
 autophot/models/edgeon_model.py,sha256=0gssPwcdvQX_XaYzJej7-6eVyqQnj0Yq1e7foK5IMX8,6850
 autophot/models/exponential_model.py,sha256=H0aX1p-6VrYNNiUPbuBVmcjkzfm4ycTcJvZQdV3KpOs,14419
 autophot/models/flatsky_model.py,sha256=07MDrRaPY693XbwL93wv_9UVakEH1O3c0FAOf5Cta8g,2108
 autophot/models/foureirellipse_model.py,sha256=3EIwbBnOA4pEk3XuTMbtcWM73pPc0VbPnvonZk8Lhmk,10650
-autophot/models/galaxy_model_object.py,sha256=RWv27DIRKSPnU2zT_6yJGK2MvSa4_pI_yJlm2sScAQg,5239
+autophot/models/galaxy_model_object.py,sha256=PbykT7MWQJ7zYqA3jLDMWuaHeOiDoi8WAcZ_TyR0lXs,5306
 autophot/models/gaussian_model.py,sha256=w7ZwwNRJyUSK-n13cCO_ZFcrLj64Tb_2E0_90DqYogg,13246
-autophot/models/group_model_object.py,sha256=uw9QO0qFonHC8at4gQLblU4yrp1sF1hIZxdUneU0Qkw,10888
-autophot/models/model_object.py,sha256=NhPDSHoCy2V0zjiT5gHiimcclEQtEMWxQ9qd3vx6Pd4,22913
+autophot/models/group_model_object.py,sha256=t5hhDwM-29I5DNPxEQ1nHeMdjDf54DC4qeMpU6uQxXU,11139
+autophot/models/model_object.py,sha256=iB425DUNRj2OJRCL4JlM8sx9uoQYqGu8PSGjCZQpAzM,23870
 autophot/models/moffat_model.py,sha256=NCHcx5MmHUdj2k4koAgvYE62MEO-DbKLhBwHNHbDb2E,3904
 autophot/models/nuker_model.py,sha256=vKNpY5rbSG7u27iC2zsVN3e07Z453ljT1NeadL8zDvE,18951
-autophot/models/parameter_group.py,sha256=tYN6R2wagPRQfq7L_HMq_qwox15n7rjsJR3GcKiQmyw,12779
-autophot/models/parameter_object.py,sha256=BsxwGv8g6wGnMV4ZWDZcvJQqYgcoz00Z5a_6Y6ZjQPs,18591
+autophot/models/parameter_group.py,sha256=ctOQfhfUUiACOimNynmweJW4ZoLVmnjdVsyLerPJkUc,12454
+autophot/models/parameter_object.py,sha256=3bxOeP7MqMkh27_fBz9SMxMQD5dA9ET-V457cdu3xLc,18097
 autophot/models/planesky_model.py,sha256=lEDwDru7LUJaGPMbqcZwoEcD4S5_nRJGsy9pzJkpyKA,2450
-autophot/models/psf_model.py,sha256=JMlsmj_dPG8rllOjeoU2TCq6OXV0FezhC7IFNbT-PZY,3888
+autophot/models/psf_model.py,sha256=HmL0qOdKqcgc_DZHB0Y1X8agQhe6bPx1efrpfJSC3iw,3719
 autophot/models/ray_model.py,sha256=tJCf9J8mHLH48natMp-fIBictTLUYm8UJAwrzsla5WE,4998
 autophot/models/sersic_model.py,sha256=RkdYJtzzz47cuyGv7YTtujeUjKb3a3O1C2J2KvOmQkU,16161
 autophot/models/sky_model_object.py,sha256=WD450x05pnwMDOzDo5yhQfqYXAHgE8QPkJLVyKJeFGQ,875
 autophot/models/spline_model.py,sha256=oqhe_37BVZh3EAmsMFMtuTt5A8FR8AXuMomPE1tx-S8,10695
-autophot/models/star_model_object.py,sha256=ahbdo31Hi-Xi4UdUAN48zisrjGiZ5NPK2RVJt7ujo1U,1159
-autophot/models/superellipse_model.py,sha256=m9Oj8eTw27yltZYoIdvXyNF9_cGEkJKXQS82Ff8tNHg,3231
+autophot/models/star_model_object.py,sha256=6KSF_AZ-Ahj3JnWPC7Flihp0VAiZj9TMD_qd9I555Gs,1226
+autophot/models/superellipse_model.py,sha256=z5tDajtolns9PQR2lrH_zg3hgHtrNuneTZny7IzpwGQ,3259
 autophot/models/warp_model.py,sha256=V0LYcbhlv3EK6DnmpGxfj8Dpb_cQzEAv6f8rfHDY4Ts,4741
 autophot/models/wedge_model.py,sha256=9YYusEhdM5I5M4VruJV2HHZNMl4ve3pylxVMexycG8w,3981
 autophot/parse_config/__init__.py,sha256=CT6gEcILfcEdz3I5nbgqKeno8tyGZsMtHmfXQUWejUA,57
 autophot/parse_config/basic_config.py,sha256=fwFhrdMxAeXYK_z0_gkjeiK6xKjetcsm0bV78o7-W6w,4147
 autophot/parse_config/galfit_config.py,sha256=D3MHk-doLnKTE_NMc1CgGEWnFGBM-vLF6m4ozVJGKls,4590
 autophot/parse_config/shared_methods.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autophot/plots/__init__.py,sha256=p8lf0eFHXeZ7-3s0SHRb3zIH1pR6x_jnMxIPUHaEPls,67
-autophot/plots/image.py,sha256=Ge-bbZXFmpUG1o7VeTyPGA3l74m9vUJ6sxENPAn0b28,14203
+autophot/plots/image.py,sha256=lU7-dd2FoPtIRJEBnc0Eo3VfIQIRoACjbsQhx0KlNBc,8074
 autophot/plots/profile.py,sha256=Ux35_Rg5RNb4qeZ5ADxJM60LBWw5OK1FmnLHxVtb0Ww,7556
 autophot/plots/shared_elements.py,sha256=-9cuoMXOSZh7uB32Fkn2R5wCFecNU3WIGvVrjmP8ESY,3048
 autophot/plots/visuals.py,sha256=hZyvNt3wY8rbZOXEtFM6nSkGPili7Mm-4RSd1fnZqMw,21022
 autophot/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autophot/utils/angle_operations.py,sha256=oJp9g9v5v7SGzuDRXILg7B0d2CcXygT3GXAzoUbtPGk,800
 autophot/utils/decorators.py,sha256=Jr7PTg_RO2IMTDy7foPjCNXMoDsWPTReO_VAlO76_GA,976
-autophot/utils/interpolate.py,sha256=0WsYxKYUny6nVezWet7zjkrjlTaPQjFqEL_03Og1Ofg,11683
-autophot/utils/operations.py,sha256=A3lXGcL7wgrd8_h9gNYnYZYFkzQIiXoksfwPQTq3kq8,5738
-autophot/utils/optimization.py,sha256=sCQ7DDH7KNeO71QgiehI2IVAsBGF1BqT7GtW86gyW18,964
-autophot/utils/parametric_profiles.py,sha256=vTHXynKTEA2CG-0tKlhv6ZUByUrrnCO76v9qo5YHHAg,5856
+autophot/utils/interpolate.py,sha256=pKldYZuyTnCHWMVkQqWAYSlmz92_Iv4myOKEncVa8MI,9815
+autophot/utils/operations.py,sha256=ulu8Tlms4S5UjfMBX8nZ1yQdjhhtHoAOSCf6C9_oUeU,6728
+autophot/utils/optimization.py,sha256=VkAusKnyc4zyztbceazKADVy85g6VV7qqYIw6V7cQos,963
+autophot/utils/parametric_profiles.py,sha256=JY6reeVkATmuPkhKY7cWPgrnpZaM--simb-8mS2xh1A,5990
 autophot/utils/conversions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autophot/utils/conversions/coordinates.py,sha256=X-5K8_3uYj5MFVK9KsuYFJ18iwA-AGCcmqsaojMnO_c,1840
 autophot/utils/conversions/dict_to_hdf5.py,sha256=XIy_JvX8Nrp-9N3sk5F3gIY2YLX5I88HxEYWbPGbfsU,1095
 autophot/utils/conversions/functions.py,sha256=YEKL5WAZvBQkR_P0DK255SIYtJoRzQmnbcEoCbweHIs,1829
 autophot/utils/conversions/optimization.py,sha256=SpxOQyC1uoaoMhn4U1V9nOrdAdyUWR59LptQ77uVpEY,3260
 autophot/utils/conversions/units.py,sha256=hqJMpEnoE6PqKNaas5qazTkzsNsGUZgg_P3X0lp-xKQ,2539
 autophot/utils/initialize/__init__.py,sha256=3loeT7k8s7wtWy_RZvGgTyxnlLKCe3AzpPUBI995Qz0,281
@@ -71,13 +71,13 @@
 autophot/utils/initialize/construct_psf.py,sha256=B79DqCOfW0rkhHlmNZ9P-lC0oH3JjMsn-A88yNYbw4c,4542
 autophot/utils/initialize/initialize.py,sha256=V1Epy8vudmquOxSB5S-moCcS7bvjdr5qze6IcqihsSs,3921
 autophot/utils/initialize/segmentation_map.py,sha256=6fG5U4y5P6MDnX4_6ROBdRhUq6TxgbwVG1V3MaLf7-c,7036
 autophot/utils/isophote/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autophot/utils/isophote/ellipse.py,sha256=p2SGzU067jBIBrKjhYKpnJQF7MSThMcnLajsaXeJ23k,1085
 autophot/utils/isophote/extract.py,sha256=ousarHmkj7GrgAZ6mO3G-QY0tXjhd8bBh6lHzb--d6U,8531
 autophot/utils/isophote/integrate.py,sha256=jNOCbSYC1dZO1pasEMcRUqZCpt43DEPVME4Hsa37DKQ,7012
-autophot-0.10.0.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-autophot-0.10.0.dist-info/METADATA,sha256=JW1CjKUZFSJkcysAVlxYnDYyv94s5xnho3Zk4IxJ4SU,3557
-autophot-0.10.0.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-autophot-0.10.0.dist-info/entry_points.txt,sha256=Rqzg1Bj83DI-80fEIqW6zkfKdKTCVN2Y5RbCoHQDU9A,56
-autophot-0.10.0.dist-info/top_level.txt,sha256=Ls8-8keele8x-yodnyQ9iPEF46A_7jDzX0pCF8yDu7o,9
-autophot-0.10.0.dist-info/RECORD,,
+autophot-0.9.0.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+autophot-0.9.0.dist-info/METADATA,sha256=KA_ErnIVWdrKbblvnk8wfxc4k3-bpTu3kEaEXkR0C4o,3572
+autophot-0.9.0.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+autophot-0.9.0.dist-info/entry_points.txt,sha256=Rqzg1Bj83DI-80fEIqW6zkfKdKTCVN2Y5RbCoHQDU9A,56
+autophot-0.9.0.dist-info/top_level.txt,sha256=Ls8-8keele8x-yodnyQ9iPEF46A_7jDzX0pCF8yDu7o,9
+autophot-0.9.0.dist-info/RECORD,,
```

